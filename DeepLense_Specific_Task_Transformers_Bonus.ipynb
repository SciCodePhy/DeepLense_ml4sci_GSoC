{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC_qp5On_ZdK"
      },
      "source": [
        "# Supplement (bonus) to Specific Task (DeepLense): Exploring Transformers\n",
        "### Chenguang Guan\n",
        "\n",
        "1. In the task-1 notebook (\"Deeplense_task1_classification.ipynb\") and the first specific task notebook (\"DeepLense_Specific_Task_Transformers.ipynb\"), we have applied pre-trained ViT and Swin Transformer to the DeepLense classification problem.\n",
        "\n",
        "2. In this notebook, we will compare ViT with MLP mixer, which is a supplement to the the first additional task notebook (\"Deeplense_task1_classification.ipynb\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWWt0KtXMNrp"
      },
      "source": [
        "## I. Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qCOx1BXGUhV",
        "outputId": "a30d4342-98b3-438c-c0ef-3f102788921b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9n0OxKEXKd4X"
      },
      "outputs": [],
      "source": [
        "!tar -xzvf /content/gdrive/MyDrive/lenses.tgz "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yP4Jr2tyo1z",
        "outputId": "f15aba84-ab48-4c57-fd24-a40558bb6399"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.9/dist-packages (0.6.0)\n"
          ]
        }
      ],
      "source": [
        "pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_Y3Gf4KK-yL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import random\n",
        "import time\n",
        "\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uq4Y-UNAMVbS",
        "outputId": "e3282bfb-390b-4649-b814-218f4bdd1c7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device is: cuda\n"
          ]
        }
      ],
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device is:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PF-R1VjMmvR"
      },
      "source": [
        "## II. Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGOwmXf4MHwI"
      },
      "outputs": [],
      "source": [
        "dataset_path = '/content/lenses'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fN_TBE83KLBn"
      },
      "outputs": [],
      "source": [
        "def dataset_path_list(dataset_dir):\n",
        "    path_list = list()\n",
        "    for dataset_class in [\"no_sub\", \"sub\"]:\n",
        "        subdir = os.path.join(dataset_dir, dataset_class) + \"/\"\n",
        "        path_list += list(glob.glob(subdir + \"*.jpg\") )\n",
        "    return path_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKTLTLRPRrR5"
      },
      "outputs": [],
      "source": [
        "def dataset_list(dataset_dir):\n",
        "    data_list = list()\n",
        "    for each_path in dataset_path_list(dataset_dir):\n",
        "        data_list.append( np.expand_dims( np.array( Image.open( each_path ) ).astype(np.float32) / 255, axis=0 ) )\n",
        "    return data_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIkcEGgMNNgg"
      },
      "outputs": [],
      "source": [
        "X_list = dataset_list( dataset_path )\n",
        "y_list = [0] * 5000 + [1] * 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "1dRLLWxSnGmj",
        "outputId": "e84794ed-7743-48db-885c-84821a1939c4"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAGhCAYAAADbf0s2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB9aElEQVR4nO29f5BlZX3n/36ec7t7hh/Tw2CYoXVGZlNU+CFBAjoZoXZ1mQoqhRJJXKwJmTWUrGZQYVxEdgOuG3WE3VWCIRCtLTW1GBOrBANbYk0GhVgZBhgkiYqI31CIkJ6Ji9PN/Oq+9zyf7x/Pj/Oc5z7n3HO7z+0+t/vzqrpz75yfzzn39vM+nx/P5xFERGAYhmGYBiIXuwEMwzAMUwSLFMMwDNNYWKQYhmGYxsIixTAMwzQWFimGYRimsbBIMQzDMI2FRYphGIZpLCxSDMMwTGNhkWIYhmEaC4sUwzAM01gWTaTuvPNOnHbaaVixYgU2bdqExx57bLGawjAMwzSURRGpv/qrv8KOHTvw8Y9/HE8++STOPfdcXHLJJThw4MBiNIdhGIZpKGIxCsxu2rQJb3jDG/Cnf/qnAAClFNavX48PfvCD+NjHPtZzf6UUXnrpJZx44okQQgy6uQzDMEzNEBFeeeUVTExMQMpie6m1gG0CAMzOzmLfvn246aab3DIpJbZs2YI9e/ZE95mZmcHMzIz7/4svvoizzjpr4G1lGIZhBssLL7yA17zmNYXrF1ykfvGLXyBNU6xduza3fO3atfjxj38c3Wfnzp34xCc+0bX8IlyKlhgZSDuZigzKEGcLmWGWNB1q43v4vzjxxBNLt1twkZoLN910E3bs2OH+Pz09jfXr16MlRlikFh0WKYZh5gihZ8hmwUXqVa96FZIkwf79+3PL9+/fj3Xr1kX3GRsbw9jY2EI0j2EYhmkQC57dNzo6ivPPPx+7d+92y5RS2L17NzZv3rzQzWEYhmEazKK4+3bs2IFt27bhggsuwBvf+EbcfvvtOHz4MN773vcuRnMYhmGYhrIoIvUf/sN/wL/+67/illtuweTkJF7/+tfjwQcf7EqmYBiGYZY3izJOar5MT09jfHwcbxaXc+LEYsPZfQzDzIEOtfFdug9TU1NYtWpV4XZcu49hGIZpLCxSDMMwTGNhkWIYhmEaC4sUwzAM01hYpBiGYZjGwiLFMAzDNBYWKYZhGKaxsEgxDMMwjYVFimEYhmksLFIMwzBMYxmK+aQKIUKt8xk1sRTPciw7VFfbluO9Yxaf5fS7m8+1VtyXLSmGYRimsQy3JcUsPk18umMYZsnAlhTDMAzTWFikGIZhmMbCIsUwDMM0FhYphmEYprGwSDEMwzCNhUWKYRiGaSwsUgzDMExjYZFiGIZhGguLFMMwDNNYWKQYhmGYxsIixTAMwzQWFimGYRimsbBIMQzDMI2FRYphGIZpLCxSDMMwTGNhkWIYhmEaC096OAgGNX10ndTZRp74kFloBvE31vTf8TL9m2VLqm6GQaDqZjleM7N48O9t/gzRPWSRsgzRkwVTkUF8p/w7YXqxnH4jC3Ctw+3uE2L5/CDqus4heoKqheXy+2CaRZ2/u6b/zc75WgVQ4dLYkmIYhmEaC4sUwzAM01hYpBiGYZjGwiLFMAzDNBYWKYZhGKax1C5SO3fuxBve8AaceOKJOOWUU3D55ZfjmWeeyW1z7NgxbN++HSeffDJOOOEEXHHFFdi/f3/dTWEYhmGGnNpF6uGHH8b27dvx6KOPYteuXWi32/it3/otHD582G1z/fXX4/7778fXv/51PPzww3jppZfwrne9q+6mMAzDMEOOIBpsEv6//uu/4pRTTsHDDz+Mf/tv/y2mpqbwK7/yK/jqV7+K3/md3wEA/PjHP8aZZ56JPXv24Dd/8zd7HnN6ehrj4+N4s7gcLTEyyOb3z6BuZ9PHSfF4JGahWG6/4SV6vR1q47t0H6amprBq1arC7QYek5qamgIArFmzBgCwb98+tNttbNmyxW1zxhlnYMOGDdizZ0/0GDMzM5iens69GIZhmKXPQEVKKYXrrrsOF154IV73utcBACYnJzE6OorVq1fntl27di0mJyejx9m5cyfGx8fda/369YNsNsMwDNMQBipS27dvxw9+8AN87Wtfm9dxbrrpJkxNTbnXCy+8UFMLGYZhmCYzsNp91157LR544AE88sgjeM1rXuOWr1u3DrOzszh48GDOmtq/fz/WrVsXPdbY2BjGxsYG1VSGYRimodRuSRERrr32Wtx777146KGHsHHjxtz6888/HyMjI9i9e7db9swzz+BnP/sZNm/eXHdzGIZhmCGmdktq+/bt+OpXv4pvfvObOPHEE12caXx8HCtXrsT4+Diuvvpq7NixA2vWrMGqVavwwQ9+EJs3b66U2ccwDMMsH2oXqbvuugsA8OY3vzm3/Etf+hL+43/8jwCAz33uc5BS4oorrsDMzAwuueQS/Nmf/VndTWEYhmGGnIGPkxoEPE5qHjS9fQzTi+X2G16i11t1nNRwT3pIhEqzZlWlqT/SYaDOPyT+Hhhm8AzJ3ywXmGUYhmEay3BbUk2k6VZA3e0bhCuCqPn3kVk8lttvYzlNRR+BLSmGYRimsbBIMQzDMI2FRYphGIZpLCxSDMMwTGNhkWIYhmEaC4sUwzAM01hYpBiGYZjGwiLFMAzDNBYWKYZhGKaxsEgxDMMwjYVFimEYhmksLFIMwzBMY2GRYhiGYRoLixTDMAzTWFikGIZhmMbCIsUwDMM0FhYphmEYprHwzLwMwzB1MqjZb5s8I/FcrrniPmxJMc2jyX+MDMMsKGxJMfODBYVhmAHClhTDMAzTWFikGIZhmMbCIsUwDMM0FhYphmEYprGwSDEMwzCNhUWKYRiGaSwsUgzDMExjYZFiGIZhGguLFMMwDNNYWKQYhmGYxsIixTAMwzQWFimGYRimsbBIMQzDMI2FRYphGIZpLCxSDMMwTGNhkWIYhmEay8BF6jOf+QyEELjuuuvcsmPHjmH79u04+eSTccIJJ+CKK67A/v37B90UhmEYZsgYqEg9/vjj+PM//3P8+q//em759ddfj/vvvx9f//rX8fDDD+Oll17Cu971rkE2ZeEgGsyLYYYZ/rtg5sjAROrQoUPYunUrvvjFL+Kkk05yy6empvC///f/xmc/+1n8+3//73H++efjS1/6Ev7+7/8ejz76aPRYMzMzmJ6ezr0YhmGYpc/ARGr79u249NJLsWXLltzyffv2od1u55afccYZ2LBhA/bs2RM91s6dOzE+Pu5e69evH1SzGYZh5ocQg3k1mQFe00BE6mtf+xqefPJJ7Ny5s2vd5OQkRkdHsXr16tzytWvXYnJyMnq8m266CVNTU+71wgsvDKLZDMMwTMNo1X3AF154AR/+8Iexa9curFixopZjjo2NYWxsrJZjMQzDMMND7ZbUvn37cODAAfzGb/wGWq0WWq0WHn74Ydxxxx1otVpYu3YtZmdncfDgwdx++/fvx7p16+puDsMwDDPE1G5JXXzxxfinf/qn3LL3vve9OOOMM3DjjTdi/fr1GBkZwe7du3HFFVcAAJ555hn87Gc/w+bNm+tuDsMwDDPE1C5SJ554Il73utfllh1//PE4+eST3fKrr74aO3bswJo1a7Bq1Sp88IMfxObNm/Gbv/mbdTeHYRiGGWJqF6kqfO5zn4OUEldccQVmZmZwySWX4M/+7M8WoykMwywEQvC4JmZOCKLh++VMT09jfHwcb8Y70RIj9R24jjTPQd3OpqegLicEVxPrG1KL3QIGaFT/1KE2vkv3YWpqCqtWrSrcjv/aGIZhmMbCIsUwDMM0FhYphukHdvUxzILCf3EMwwweFndmjixKdh/DDDNCchJLv5Ci+oSKkzCWFfx4wzAMwzQWFimGYRimsbBIMQzDMI2FRYphGIZpLJw4wTB9Qio/ap8TKTThfanhgPUejxlKWKSY4acsa6xXR1dDxhkpmrtQ1ZmavYidOgsUMyhYpOrG1rCqs0bWMNTtq6Ozrdox9XOuObarSHRq64z9dtVhidUtEha/nTUIR+y+1i5wTDFD2D+xSA2KYRCWJcR8Or9SK6hCJ923FVW3QNnjKMkWCFONIeqfWKSYZU2/AiOk6M+9V2bJLaFYlr0vDFM3LFLMcFBzWZ3K1lPZvlXbVHIuUdMTrZtxZ5Dlh+yxe1iUtYhVj3MxywcWKYapSi8B6GEZ1SVIRcemuvWpSGzEAroVF/JcTCNhkWKaTYkw9HK5LYZLrqcQyQFZOkrVKoJE1H3tvmiF98wTkrm4ULPDRISRraplDQ/mZZiFYlACNehjM8wiwr9spjksxnQOg0xskDL/GjQ1nqNvq4yn4mAGBLv7mGZRobObT9JDKT0SHKifsSW+YCxUui9Rb6FS1V1mVqjcdcfuT5kLsBcR911p8sWghJDdiI2GRYphgFrSwZ310Uug/PV9iEaFBmSf6xysWYYUgxtIzDBgkWKGhJ7B+EVOBy8VqIh14yyzQbgBleoWR2cN9S+QZfclmmDhjt9DvEoSInomUzDLBhYphqlIzOWX68CtAMSWhdtG/l8HXcJnhShmZUlZryU3rHCae6NhkWKGnwW0orq262E5ZRZW5Ph1x1hIQYR9bZLEY2lWnGKWXJ9xq8JYXVVXIIsEUwKLFNM4ahnfBMxdlIpccGHnHVpOvcQpbG/dZZGUBGTQRkVd1+qsrSK3YJ/JF9HjW8rGWuUOUs31NwjYndhsWKSYpckgOjYpo668vMsvKJmUE6mgTULW2840BSg4XoIuYRAUcQEanMBYIYpZSewmZBYQFilm+OhzbNOcrKZs58J9SsXJLhMi3l6zvRCinhR1IlCS5AXJilEiMrFR5AloxGqx2hSztHIWUnBNnmjF7nc0jZ2rSzAVYJFimH6QMi5OQN4yEsUWldtfiHoH4KZpVr/PFyO9QL/lBCsyDYkVTwWwE4xpAixSzNKhbhdfJElCJIGo+CJUVZgsSVL7QF+RpubYYUKDZ2UJaFHyBQuBFZZAr/PdekIUj79iFyAzIFiklhtVBnnOpeMsc8/UmcVWdKwCgYq6+sqslyrXXpYAEQhUlzj5yRZClMesquB/n5R3HYZHy1VJd1aUF3uybSlLJCib2dVeW1WxKnP91ZzxV//09gP6Oxo25jNovOK+LFJMo4hmcs11CnjrTqvSWYRxprKsPLvexpuqCBIAJIFAuTmgBCip2M5QJIggUtW93uJ1BMJfppSOY5FNE1ddnYYoaVPOSvNFyc8czDW7JAMQKHjISMzxCzqzxYpbLVQ1DwbAsItUXUFnpjdFQlFDR9ElTCWJBv0fvFrcR/hWTWx699DV5QlULvkhNqDXHltkL5ICQpE+ZiimZeOLpCdwdn+ViR2SYpECkG2bJECaas+fBJDGrlE6ndDH8pIjnAEWiJI9Ro80/n4GMkfnyQpjbvP9HZY9CHESx6Iy3CLFDD2DHgMzL7qSIHzrp2QQb5dFJXJiSYl998StpeNTJISem6CoX/S8dEJqkSInWkb4fIiCrDxzbE9cBACy7jV3/oLMO+q2kAqFimFqgEWKaT5zELJ+ntRz24ZJD34yBAUp2daKAvKWUChOZj0FYgXjTiNrAfkiBXQLVZisZwVISgjjwqPUizGZdydcLg3c7O+5HQUASr2Dl1kvpHLxq76Eag4JFpUq0C/obMEswgsJixSTUbfrdB6z6joGVBC26xgyLz5mZbCx1xH6br6wNJK/vzkG+e4+77O1pqgls2URr6PFX0dGpASU9hpC5l2BgLG0zHZ+H+5bSzE3Y2x8lF0f3IcuoQL6G2PlUyBglYUK6H8KkLlQlkDC1AqLFNMs6swE7BGLyglUzIKKVY0wQZqcQElvOy/uBGSuPUiZt5wk9ODblgQJOJEiaYQL0NZRAJnjipSclSQSBUEAKaXdgNbCItLbKQUoAklvuSLA+z/Q0vulqU6mALpT1gHPwvJS1s2gYIFM5Cgsclu39cFThCwbWKSYcguql2gMYpLCfqtGAMWCFA6+LTtXzIIKM/eyBnUnO4SWk93OWE+u47axqERAtSQgjJhJvZ/dl7qSyP3zQ5tVqREFAkTqWVgpQRCBoIyFRYCXBUgSecvKc1UK2IQIT6xCd6dd7ycwWDegb1kBxXUCXWN6WFmRZIuuKUJ6Tb5oRDb2W+xpXZVVwWCLauCwSC03YllqTaKf+FOsQ6t6PWWVIjwLKjoI157bClCQWp5z7VnrScos7mTe1UiiLSgJKM+iIglYfaLI9ej4k9YdoQiyLQACRCq1lUSASBVIEUQqtMVkEiQAaLHKuQGDmBURhPJS0t3AYH/Ab1DBIiy3FBS2pRLxKXQLltDl/puHZSWk4CKzDaZG30rGiy++iN/7vd/DySefjJUrV+Kcc87BE0884dYTEW655RaceuqpWLlyJbZs2YJnn312EE1hYlTtyGtwvfVlRVUdkOu72PQGOSvG386llocvIbNXmUCFbj0pdfq2dd95yRCUSGMRSW+91ELU0u/U0tuokQSUCKgRiXREIh2VUGMS6ZhAukKis0Kis1IiXSHQWZm99P8l0hVm21GJdCxBOiaRrkygxhKkY/pdjbVARggpMe+2XV47XZutqNprtBUxhIAw91LYa04S7x5G7q397H2Hhd9j7Lss2q5kfq7SB5wev+VKMdKyYzTtYW8JUbsl9ctf/hIXXngh3vKWt+Bb3/oWfuVXfgXPPvssTjrpJLfNbbfdhjvuuANf+cpXsHHjRtx888245JJL8KMf/QgrVqyou0lLg7piNSUVsKuea85p4xWvoXJB2Fhn1m87Slx8XanlsXP6nar/bsSLhPcujbWUaBcfEgGSOhVdv/Rn1dLJE258kDDFJOzDvslPEAoQZhyTjknp70YogoKEEARKdVsEAChjVdmYk5fqTlJAuArq1H3tXlzJfj8UJl9Es/nyLrnCQb1htYqyGFZZhmDZ1CA9xvpVmg24iuuPqZXaRerWW2/F+vXr8aUvfckt27hxo/tMRLj99tvxR3/0R3jnO98JAPiLv/gLrF27Fvfddx+uvPLKupvENIE6RLbqBIO5fUJRKRComAVVNDAXcG487b7zkiOsONl9W1K796Tsij+lI9p6IimQjmk3YToKpCPCbJcJlcvqs5rQAWQqIFIgGSUtXB2dwScUQc6S21YK7c6Sxk2oq8cal6HI3HokRL56RerFkcLYi3UfJiJLXReAqwmYopuSquv60AMYb8UJFkNP7e6+v/mbv8EFF1yA3/3d38Upp5yC8847D1/84hfd+ueeew6Tk5PYsmWLWzY+Po5NmzZhz5490WPOzMxgeno692IGgHXfRFcJ96p0jPBlibneAtdQjiKXkF3nFtllkWMWtcM/llnWNXWG30mL8LginyARJE9AwrOkYJInjNWUewdUIqDMO7UANaLFSo0IpKMCatT7PCKgRuBe6YiAakFbYIlnmfmJGMK20b8W73MYT/PvZfQ+yILtZH5Z0fdQVoS3iivX26folfuuq/7e7HF7/d5L/laYeqn9Lv/zP/8z7rrrLpx++un49re/jQ984AP40Ic+hK985SsAgMnJSQDA2rVrc/utXbvWrQvZuXMnxsfH3Wv9+vV1N7vZLESB1oWggpuwZxZfL4FyCyPCFLbDWlBheSMrirmYVCBCMSEzn93gXJMsQVKYeJB5T4QWo8QIjxEfHY+Cjj8dJ9A5AWhHXrMnAu0TBNrHCbSPF+gcB739CoHOmD6ecmImQC1t7ZEUJgZlLD3TVvIswPw1+3EqkbMe3fokyUQhF+8LjxURh6pCFaNqqasqsNg0mtrdfUopXHDBBfj0pz8NADjvvPPwgx/8AHfffTe2bds2p2PedNNN2LFjh/v/9PT08hOqOomNzp/LwNuFnL4992Qsu48RPsn3OH+piy+MLYXnj1kdudgTPAHIBIJaZpkRLjWi408qMVZRC0jHBDortZtPjQKUkLGIjKvPvJJZbU3JVFtOQgGJIGM56W2FAKQRREnGzUjkUtIJEsIWlrU7EZm6gt612vqCRMhVw7DLkiSXPahjVeF3WFBtvVcFC6A7CzCMY4WUTMAYndo+TKMPKM3+W8hKF8uU2h8hTj31VJx11lm5ZWeeeSZ+9rOfAQDWrVsHANi/f39um/3797t1IWNjY1i1alXutdzw3Q/zeXkHLLc2vPM2iljHNBeBqrJdkUCVHc/gxMp3DQrtdiPzcp+l90qydzVCxo0H7QK0n0f0utyyxNvXe8G4GeHO5197pP0uKSJyzb0eLKpYLr0eLNxmFb+/Oqhi4ffKHKzwt8TMjdrv6IUXXohnnnkmt+wnP/kJXvva1wLQSRTr1q3D7t273frp6Wns3bsXmzdvrrs5zGJRp7j1cvEVxZvKjpVbFIl5hB2zv40fi/LPmbPA4FlW+v8uk8/938SQpDBCpBMn1BiQriD9WklIj1fonJAiPUHpz8cpdI4j90pXAJ0VMPGqTLhUy1hr1r1o3G2usK3Iu/3Ca7fxrHxJJ9ltYQb3rivtH8h/R7F40Xxcf31QRyktZmGp3d13/fXX401vehM+/elP493vfjcee+wxfOELX8AXvvAFAPpHct111+GTn/wkTj/9dJeCPjExgcsvv7zu5jBzoFJyREidrr38zl3bldbbC/crPK4Xh/LbUiRYRRZUuFyG663lZDp+z6JxGd+eVQUBqIQyy6hFoES72QjG3UYApVlWHSXeuQVcwoYQpkK6SwkUXiZftjh6z/zPzgUY1vpDd60/cw6Xpu5XpgAy91hw/Nw6b/to8dqwfeFxygYOo4cLsEL9P72aMwYXitpF6g1veAPuvfde3HTTTfjv//2/Y+PGjbj99tuxdetWt81HP/pRHD58GNdccw0OHjyIiy66CA8++CCPkWKqU+IqqiRQVY5f9ak73DYqdN52Roy622WEShphswKVENDSaeZQQotUi3Q/2hF6G+jYlkx0/IQS02cL79ywQigg3Fgo27FDp41L0V193QiPi1WFcZ00Uv6IQpHxhKroHnLHz0QQ1LO0cPOYnp7G+Pg43iwuR0uMLHZzBo+QtcWGqjwBzml23Mg+c7agYvuFlpTvIvLPXxrL6N4+V/bIz2azy7wkCX+upsLSR1K31RaMVaOJS5pIR2WWep7AZfapEYHZE7Srrn0C0DnBxKKOU1qARhRESwEkQMqIyKyEnJUQHSCZERAdgZHDQHIMkG3S7ykwclghmSHItkLrWKpr+pm6flnxWUCkafaeBipVNO2HfSnvM5BZLV7X4roZ//dH3dvpY6ncdtEuqmy6j7IuLbIf+VXjo8eL7NPn9kuWechHh9r4Lr6Jqamp0jwDrt03JPTrXmhcwoPPHAWqUPSiCQ4lWVdzuDddkwmGkCnoqowfzeuUBQAiod1ryrjsUqH/+owxI4gglLZURGoMi9Ss7wiQHdWbanNLdAREBxBmQK9QMCIDM6BXt9m9p2SKziITKFcDMKjlWORKEyZzUAXb+xUqfNect6x0qg3P8nLMx7KKHW8h4Yy/WmGRGgZ6/eAjVg4pmpNQ1WVFFW/rHatAkCrVZCuanDC2TcGxolZUSQft/uu3J/cUrnRnbFO7hdDVHhKhDy31uxICkOSVIQJkBwAIyay2ulQKJFJCtbTrDol294lUu/vkrIDsGEGbNSnos0AyS9q6apNe19aCJH0LqqOci65rOhDfkoxdo1+Y1lpcJrU95yYMH0T8grXuXD06cyNUUYGLHT+8jn6EqkwUYzMSl6Wl233KYBGrDIvUUkAXbau06ZwLadZhmfWoGqEXi3JRirUptKCKtgvPE+47VwIRE0RZ/2gCQ9aCIZjOk7TYWCsKJDILKDV6qYwoIdtepNDbpma71NvPWlPuuMZqsuOrgMw1B0+gXMIBQErEJwgJkyT8sVM+/rrocQIxKBuzF0z9EaOr9h8w9+rqRb9xfxyVO56a00OgEzb/eMMsWPP6+xHZ77IEFilmIEQrXmcr53+CKjEof7u6KUhNj+ImGAQEtIUDALJDSNqAUgKJBFQCJFJn5MkWAAhIb/yT8ERKtrX1JRSM2w+QswSZ6uPaOn7aBWgsqH5dYKGI9bofVsii7jvpzk9dzxEJkNqb4i22A4IjNf/m7Aqcw/T1zOLCIjXM1D1wsI6pOcqSHsJ077m2qaoFVdamOgjH+5hlsTmggMyScpZOStrNR3CTEFJiBCs1UzW5sVW+SGmBkh0AyooVOfefjUvJ1LOiFLRFUrVvn2tMx1pKfozK3QCRS0/PTuVbKrHKFLHfQGTOqqKK6l379lhfhbJq6EytsEgxg6XGgZj6eP0J1KJDOvZERiiEJJ1EkeqYoXXtSZ1cB9mBzugjkQ0Alpkl5SY77GTi5ESpY6yo1AhUapMnaO6iUzcRF5wwCRkAPEHyOn+3qM9rqFhRvTSpg1l0WKSWKH37ywfR0Re5+AqEqyseVTQpYj/jl8qou6JBkRVlkwykMOXvlBlsqyCkhLRhitQIkBIgSVBtT6TMcYWxhoR19RmLCQQkbbgkCTmrl8m2guioTKyA7nFQcyRX469ollw/RtVranii/CzAgLHKADcFiD2Xknk3YNFcVXVP/cEsOCxSw0idtfZqyNwrdfHpDbqXle1bc5JElLoFqggFHY8yCRMCwuS5mOy+XHKD+b8yyRbGtScUQDamZftbpa0vt4yQpZyn2f9tAoULUNfVSYcdfvj/XPHWgmSKsIhsWbUKP8HCxb+8Zf73Xlatwm9nVbeffy1d94HdfoOGRYqZF11lhbo36FpXOUZUVuqoYq2+uAhWXBbDjDEif8Cvvw6Z1eOPnRImqYCEMQw8YyHL2iNzKdqS0uIkXLkkX3CsRQWyYkWQbf3SyRTKCBa5dvQtULEBuGUUjXfSK3Np7F3bR6pV5I7RlWFnlbk7665nWaUuQZUQSs3P5cdjowYGi9SwMJ8p3ft15fWwRionRwTrolNrdB1cBi6/EksrNuYp3uDi9vbazsfrxIQiXag1djo7qNc/hR04a4YUCQhnQRHgLCI3+JaEmUSXukUKnvXkWUyyY8ZCGZefb03VUnKoapZf0b6xpIoSoeqyqHxsCSefcOqPXkIVXFc0CcMnYq3lr52tqkHQ4IgzMzRUEKj8sj5dcrllNQjUfPAtklwpoMBK6ON4djxTVnnCc90Zt16ukoSfvZdm27l0cydgnhVlsX10LL28at86F6vMWdSRB5CYNT6X7y2s5zjgiurMwsCW1BJgIUsgRcc/FQ7cjLj4eg3UDasdxLbrJVBlHdxcLKhe+BZBmSVhXV62/cYSIhCgdNVydAQkzFgpbXI5FyHguftc7ArOXShTMz6KKCv6St5+7hiBQHnVInqOpYqt73Xt4X5F8aoqFpU7p0RUVf0swaoV1SNwxl9zYJEaYuY0pUYZc51uAyh9Qi2c3r3o3GXtLipr1Ed7gsZV2y7Eik4R4emLRNqUryrrDgUhm1rDT4SwApQacQlcUNExURLaVZa7jhLmE4+qSgWh0puJiFB5UBDT6mfqj2izIuJoKUumYGqFRWoYmEt6+EKOHYqVNuraJiJCVS3AqmWN6nTl9BKvKuJmU9LNhIO2Mro/6WH3Pt2fKbaZHR9lY10UiJIQIJBuphU5vwO3LrtS998cXZjm/KX4MSq7fUWhApCvWhGbq6rEooq2tc5KFJxEUSssUkNCZZdeRSuk8nl7uc7s+J0uN2APt17RmKcwcaKsHXXFMnx6zSPVa104Vkqa5dKug5kZF/DnefKx08zrz8EprDVlq5cbt18uzTxyvDJbLeriq7osd6B8Y30RFn6f7YtRP0Lln8odivJWjS9UJRaVOzaQiVOBUFWyqHzYuqoVFilm3vRddqiOGNqgLKiK5+yaT8qsJ/sUL6X5LM38URJqROoYk0krp5bUdfmEALXMewIoI2J6EK93eoIb8AsBVwLJuf6kyRZ0oiSy1MFcZh15x4xYS0Wf+yBuJRbErhJPUFSwrT+WCuhKI4/GjkKLKhhnxfGm4YJFailRs4tvzpMWdo1vKhjTVGJFVcrY61W0NrasKPBfkZw42X19C8oKVKKXqZa+JmoJqJa2qKwIKTPhobWu9DugWuazBOBpii19JEgnViTQ20h7Xbb8kmssAKn363L3AWaoUYFLr2IppagYWXKinl1Dbn04ZiomWD5J4q1XeaGKWVT9UuL2Y3FbHFiklgo1uvnmlCRR5Ry9xj9VOX9VMaoqPP1agSK4Bt89KBERLxjB0dvaMU8kdWzKCVZiliX6M4xg+QkTMGOsYMsmJUaAEpjsQICEV0TWcxl2XWbMWvIzDwdEKGq5skpdllbE6grdgWUWlcV385XNUeW2Lxcq3QwWq4WCRYrpj35da7EkiaioVDhuHS6+eWShkS9IuThTXpisFUVewgQFFhRJgXRMoL3SLGsZ915L5ERKnxiuuGwyq8dByUSYWn1wFdRBgLTzVJn9kBiXoBFLf7aoXBUKVbNQdVnJ8WN2WVj+92MnU/QpMY5yFlU/cSEps6lCmMbBImUp67jmG4xfLOp8Kp6rOPntmKs4hfuWJUlUzLqL0s8g49DtZ6woK1gkbAJEZk35FhRJYz0ZcXJuv5YWLGtdASYW5SY0FG6wrzLWk41R6Q11Vh8EMjkqGFLkqDPQ38934llF1sIS4XxU/r6+Sy/10sjtNr0sqsAFmLOKwoeXWByMWRRYpIDeT9a9BiouALGpqgcxiDc6Mj927VVFKxSoHm5J4Xf8RecsWl81BhXbtqudJcIaHtsJk9AuuURm6eYtbUGpUYl0TCdKpKN6WWcl0DleW07pmIlHjRLUqBU1IzYd4SpNJMd04dlWSyBJzBxSJqEiHbUdvW0fdO0++38pgFTHs4SUIChd5DYRQOp1xOFvrSCRQkR+k10TGs7VcvWEJ7oskcbqk1mChR+jktDp5wJwFdTJmJZIB5uBJ0V+LJp/WTIy7TynrJfCItUEFmE+pMqVy73llWvv+duE4tRrwLDJiCvYoLttIb3qq5VtHyZEwIuhhOeMucVc0oR+QQJqxLj6RrV7TyVAZ6WAammBah8PUIvQOZ6gRgk0piBXdrQ4mcN3ZiUwK4FUIDkiITrGlZgIyFktUFLCxaJkCl143Y/3mM6eBCASYeoHSjNPVcRF1iMFPSZQ/nJ332IiU0KXRRVaUvY4NsnCLvctMKW0NvljpQBPsIxQAV6sqiAOVTDwt2cSRYnwsFD1B4vUfIiJS9UfWmTfmGUUs6D6pe8svYg1VVg1wr/ewiy+vEBVHXu1IMw1BuOLl02YcPEouMG6NktPx6R05p5qmdjTiLGcWkC6UgEjhOS4DlasnIUQBCH0BIkzSQtpK4FqJ1CprqiuRgRUGy5xwiT3GdeiQJYSiKyd8Dp5IPt/DNvhzzF+JxSVZ/7FzuWh56oKO/KIG7CkfaUxqpgolKS6Fx0/hhOvkv7B/1t3f+ML8bA6hELIIrVUmasrsKpAuZUFsacyYZqPCFW1ovohV08vlmUWnN+LR2mLJjFWU+Klm+s0dDUioUYEOmMCnRV6PFS6UgtU+wRC+yQFGlU4bs0RrBhtY+0Jh7D++F8CABJjGk0ePREvHzseh2dH8fLB46FmEnSoBUHGEuoIM0WHcNN2uKxCYVyoZN18yOr69WIeGWxdAtXrO4+s912HTrBCy8p+V8aVCSCzfML09CLrr5dVxCwqLFJLjfnEqfqxoIrOaQWqV429MpdeVRFbwMK6MVxihLOkrPVkLChhrBxpM/cyS0q1ADUC0FgKuSLFiStncOLYDE5dOY3XrngZiVCQQkGRhARBCsJIshKvHBlDG1oQVYsgUp22TkqAhDdflTAFa61VR0DPu9WvqzR2T4oG8frvlQ7kPSRQt1XWlWDhH79KeroUxg2Ymk2Ne3cOaenMYGGRYgrpKVBFSRE2AcJfV5T0EI2NLZL4RKwn57YK14WfPcHK0s6tKAk3cFeNAOkooEaB9DiFkRNmsXJFGxtW/RKnjB3Crx03iXNWvAApFBIopJA4qTWBVa1XYf/MKhw8uhKHJeHo6AjUiG6fagmATMq6cS+6sktl1wr0rnrubTsn+klE6QOddu/FrarGvmz19BIXXtUCtMzCwCI1pFTJ7JtTxQgp4wkSvcQpdOv5CRBRS6qgbf12YlXryfXaLhabMsJUGF/xY1HWijLp5TYORYlwVlM6at5XkLaijksxfsIxrF55FGeeOIlXj/4Sr1/xPM4blZBGYRRSrBLPYZU8iudbr8KLR8YhBOHYijGoIxJCmZJKlFlswgz2BQnP5Qe4GFRYZWIOIlQp3hQTqF4VKmLVJgq+w5xQAVmqvS8yZp0w23dZiGF7eg30ZRachU8rY+bNvFPPq86pVLdASe9ll4WvOpnrfYp0UMIvE6QiHandr6xvE92fhSBIb6cEBAmBREgkQouVNLGplCSIrFsK+Sk7ynIgYgkQMSMh5uKba2fd73fZZXmXuA09otmXEbe1o+Jvomf2K7NgsCU1ZEQFqp+soB4C1eXi8+NLBR1PqWvPP14Vl08YGA/pKiYq+u9Iy87fK3HCqxsnpNRP3MG2bvCt0OtErkqs9wJAJNAhiZQkFAmknpKlpKBAOEYJDqsxHFGjmElbmGm3gI5ORZcduJeICE/XZIe+2A6q4kRMMIqSUMr293+qsUrpBkqkfoiQ/qhlE4jzav0JU1WCrMvPjqUqyniLTBXCLDwsUg2n1GqqKk6VLadgLJN//DDu5PapEHsKqzMUnd/g0o/DbWLCUTbQus8xOr0QikAiOJ+L7RhNApArS4TIZw9FgCKBNiVoUwttStBBCkUEBYWUyC1vU4KO0mIGUwYJrgJF9zn8eFN0AsTgGiovLyMmStHYZh8xKr/SRMFsviSFHrjsC0q4nZ8JqJxvMH8u/zwAC1UDYJFaSsQy6irvG4iPn0JedKyuwaw9LKiscT2rZ1PS3Sl0zUlkz9VP5xGzxCzWiggyy6LbunXe+Yl0RwlAdBTQkjolXBhrpw2AADkrABDEsQSvHF6Bdprg/1vxKhwcW4k2JXhFvQgASCGQksTTx16Nnx45Bf967AS8fOg4zM6MQBxLkMwCyYw+rmwTpJm6Q5hp5GHKJyE1FpOdd8rORVVUBd1bXjRoFyhJM+/luosIFEV+Y8Lf3lp9TkS8df6YJosbyIv89xn7vRQMpOXYVDNgkVoqVBkoG90vSDeXIi5QkbhUNLW8zIIy71QmfP4xgg4lK0YaiEhUPOaIL1R+W2LbJcaiUQAkmXaZzl0KoKMgUp20kImU0O8QkMcE2kdGodIEPx9djanZlTiajuAX7RMAACkkOkrihaMn4cCRE/HKzCiOHR4FzSRoHROQM8IJlOhocRKKsjp/Spt22USJ1v0YxKhi03IQlQpUF2VJElFhML8F37UXWKhkZUqhO1HRrzZB+vjaAs9fQ+78xirKlU2SlE3tAXSJld7Wy/QLJ0pkBg6L1HIk9tRaFjMoE6ii4xfUwqssUOH/PXdNrhpBmSUltMVSSi8xKrKkfKvV71DJCoPuPEWqb59ItVABBNnW28pZgXRGIlXA9NgKzHYSKBKYVfrPsqMkOirBL44ej+ljYzh2bAR0tAXRNuLkLCf92b60UGmrTtfuI4jUpF1by8KKVVEppH6EPuo6jnyPYXZekUD5xywbB1W03HfvhRl9sYrnsYG+0SoV5VY7W12DgUVquZHraIOafKEVBaBwIsJYBlWso4mJU1k8IuYGjBUy9dyBwndb2eP222GUxbZ8gtiaIAIR6ekwZGCdAJBtBaH0fbXjpkAENSJMRfMEqpXg2LEER1uEX46dgOfH1gAkQEonVtBMAjErgY7AyBEB0QFGDgskR/XUHcksIDukP88oXYh2Vmmh6ijIjtIWhHX7BWLlRMvd5zl2tr2+X/dbsL+xyDJ/c/udCMq+TpXt49ZDGWtWF83V60KhspdWELeyolRHDT2ePr5WWKSGhX7revVKlogVjXXrSiytyDG63H2BtRSdzbbkXN1iEQS/geLOxj/GfJ9sY8LlW4lhG2znL4VxUXkxIEWQqbbsZMe8t7XLTqQEkhLUIlAqkM5al5IwMSypracUSGa0SMm2yehLrZvPuPNUZkkhNRaUUnlRsjEpP7Ei5vLrx3Vc9uAR/Y2hS/DDATGkjBjl9jNtVNoSc8khtixS7DdhhcorB+Vcfqm3XZlQ8eDeRYNFailQR4WGmHumdFqNgiQJf5kQuhq4vyyW4RWe1+J3QEA+0F+HCIXYDs2fxTU8H1G+9p0iCJLZ9SUCwmgVlIAUyiQY6I6PEi02KjFxqlmdJNJaocVcF5+VmauQMpeeUFqchAKSY6STJjqEkaParZfMkLbcUoJoG0uqrSCUiouTFSb/VYUi8cpZ5IGbN+baEyZBxm7X9UBA7muBFSz3fVA27YgvRkWZfWXt9vGFSiq2ihoAi9Qw00+V6Urbye5sPBmZ4ykUqFinFBOniLswltUF+K4cj5jbBuiuPDAf8fLPW1SFwn/3O0Mh9PQXUBAdAFJAQouPFgdpUqV1dqPsAMmsFix1VF+Hnc0XVpyIcinm9rNsE5K2cfMdVZCpEShFEB39Wbv40szNF7sG6+YrEql+rCkg+hDSJVBCuLm3sv9n+zoLySbtkXb5+ckUzr3qZ/nlppuPWN/WjWkTKAA9ZoryEyfm4lEmGYPjTYsHi9RyZD4j54tcfGXvIbaTKiHnymkCgTi5jt2zFgURSCkISGM4ESBMt6q0oJAgACarzPi3pARSZcopJWTmeDIdsRUmsp+1CzGZ1Wnmsk1e/Mm68yiLQSnE3VSxiQ3DZUmfAuXTS6CALoHyfxOuYjv0ffSnvZ/XQ8ggLHBmoNRe5yNNU9x8883YuHEjVq5ciV/91V/FH//xH+eeRIgIt9xyC0499VSsXLkSW7ZswbPPPlt3U5hBEcaURP6Vi0ElMrcNST2FBSR0Jxi+JNyvkmKit5iVz0nHeESqdIZYJwXSFKLdgWh3gE6qx0eZl0h10oJop5AzKZKjHbSOphg50kHrSIqRVzoYm0oxNpVixcEUKw4qrPhl/rXyl2adeR+bUhibSjHySoqR6RQjhzpovTKL1qE25EwHsqMg2ylEOzVtSPWsu6myI4czMfKtKO8a592Jx6xk+90L77sWdqoTaX4zgF86y9VCTKSrjah/A7K7Aj0Q/M4iv5PF/O0wc6Z2S+rWW2/FXXfdha985Ss4++yz8cQTT+C9730vxsfH8aEPfQgAcNttt+GOO+7AV77yFWzcuBE333wzLrnkEvzoRz/CihUr6m7S4GjQBGL9jo/qezxVtmP+PVjelSRh14VP0OHxfIica89ZVP4TsA2S+7v4aen9Eku48F1+9pzhtna9fdTzU7uJTOBfmZiJ2UQAsPElAoiUXmZmyYWArmZO2vLyq1e4qhFEZvCuguhkQuPO48efes22W3ZP5pJIEXPRxv4vsuK34b62ipTw0/qtK9AOSPZcvyTMlmGKu61C0bPNNWT0NYmldC0YgEj9/d//Pd75znfi0ksvBQCcdtpp+Mu//Es89thjALQVdfvtt+OP/uiP8M53vhMA8Bd/8RdYu3Yt7rvvPlx55ZV1N6l5VJyVd0Gw500S0xCBrjR0t63Mx5fCOFRoQZmn5zCTq9TVZ5+OFYAELsVb70j5TsezCBxhzCgsc+Pj7xcO1gzb5MQqOLd/zZ5bTbS9a3HrTWcqjWWQGKtSCC1Itk8298eKbs7taZJH3EBdb/yTIOUSO0To4vNER4QJE/Z6wnvnC7B/L2LxQv9Bwo85WasHcO5Dax3pGobFvwUnVlKn4+fug9DToQCA6CgQdNajS0f32+lfz0IQG2PFzJna3X1vetObsHv3bvzkJz8BAPzDP/wDvve97+Ftb3sbAOC5557D5OQktmzZ4vYZHx/Hpk2bsGfPnugxZ2ZmMD09nXvVSuCuir7mdFwZf0H/4fmv0u0HNa20HRdlj28FyqtU7pImQoGS0nQ03RZT7pWYbaXUcyy1ZLGLz7gAKZGA2U7PyySN2OlOPXP1FHw/oWXUy03od+j2M0U6bSDvLosdk0i71jpplrSQkna92Vc7hWhrt5w0y+RMCnlMv5KjHbSOtJEc6yA51tHrZlLIo/r/yUwKOZtqt17qtd0eO02zdgRp5llKuucC9LcLBT9wD0atsdi+RpxD956L4cnMLacnioR75XAPQhH3YSLdb8udq9d3Hba/xodDIbx51AZA2Gcs2oPtAlO7JfWxj30M09PTOOOMM5AkCdI0xac+9Sls3boVADA5OQkAWLt2bW6/tWvXunUhO3fuxCc+8Ym6m8rUQM73H4iH7TRymVy9iG3jZ/XF/u/2o+xzkWsrXBdNKvCsqvBp3D9OYF3peaf84+h/rNCTLXQgzZxFkBB2fYyYhegy3gJhIGM9papreVcGH1HcCq1CrK1FLuAIJOBZjGahhEsxz2Fjk7DljrTbj7qqQ0CPd4r9LmJtjQlVGt+cWXxqF6m//uu/xj333IOvfvWrOPvss/HUU0/huuuuw8TEBLZt2zanY950003YsWOH+//09DTWr19fV5OZqhR1Qr7lJIWzeLrdPuh+co1VOzcdkfCLt0qAVDAmBgBQIiKxzmgubpiYUOXiVqYdgEmX9gL3lFmb7snXjvkRKhsnZNvsH98SCpO/PnTXFWUhhq698Bx+G0J3XkGJq679/M/u95Ctc9+/iUch8eKUSfchXUKf91WD8o4F/TuJtD+GGwOVXYeADkEsubjUEqJ2kbrhhhvwsY99zMWWzjnnHDz//PPYuXMntm3bhnXr1gEA9u/fj1NPPdXtt3//frz+9a+PHnNsbAxjY2N1N5XpQVYuScY7Jr1Rti6woPxOKhwH43Yv8mT6FQSAbheeG/OSbZcrMFpkARURtZRUFqMKra/QygrOJRSyaefdfvbavNOYJ3jyr8muK3LBAfGEiDDuFCyrnETRS4TKKNsuNpC7zDNnxR2UWVtCj5UKhye45ImudhQJFleQGBZqF6kjR45ABsHnJEmgzA9i48aNWLduHXbv3u1EaXp6Gnv37sUHPvCBupvDVKWfTih8araxKS8GYRMmbPyBvFiBH3cInoNzGWyuk0qVG9CpK1JD/3JTyjorM+kdiaxzznXUZYJVFGexQhXbNk09cTaCRXnB1jX7smXOuvJdS64jLiEUHr8dZTEioDy5JGQu8ZTwIQXIu3nttdtBykKAWnqdjkeVHNtl9Jl4jx1bZhNF/CLDkoyFmrcCc/NMAQjLIzHNp3aRuuyyy/CpT30KGzZswNlnn43vf//7+OxnP4s/+IM/AKB/bNdddx0++clP4vTTT3cp6BMTE7j88svrbg7TDzGzpqDjymXyuYB4YEG55ArkXWBAYcqO87iQESwifQzXMcHkbgsgASg1Fp+NR3hiRBLdE+H1Q+j2Co8RCpb/cObFUJwQdWUF9NGO8PxFLruia53L9Zc1qaCCSOz34i5bZGnn/rio0ttiXXLIvnM7sJmE+cl64h0dBB7eE//hIqyIzjSO2kXq85//PG6++Wb84R/+IQ4cOICJiQn8p//0n3DLLbe4bT760Y/i8OHDuOaaa3Dw4EFcdNFFePDBB4drjNQSp6sUUrYiuoxCsbIxKCtQAlkgvCwrScJ4aIwbD3r8E5E9t7aqCLraNxLoBAQyqmbELNfeXNaZ6H6S9oXNd9lFOvywPI5ziRLFEy5S6r5nVbKyYm7OKlZRr6SQImybwzbY+1mW+BIKlEQ+WUZmlhMkMgvKy+wrROh/BEz8SRjrW3g2eO78lLfuahZnZuERNIRFqaanpzE+Po43i8vREiOL3ZxiSlLHo+mjfVc6z47RlSFWNoWGlOia4DAxqehFqed2e5vua118rUSnlQuRTbdgXTu2WoDfOQnPxVPUWdvxQTZhQNm5j5ANXO2o7LOtTZdmA2ldBluadtemC1OulYqKlE/ZLK2lc3HFKPtuYsuiMSkVX19gSRWKa+y8Yfs8V637/n1LSkr323Du3pHEPbBYV68a0Va1SkTm/iv5yTt3r/fdSzN4WXTsHFnQU5GY34QrB9VJs+/cun5doV3v95Cm+t4oAsj7Xdj/m88U/E7KHgDc8aIru/frylYsINZnRPcdkgSQDrXxXboPU1NTWLVqVeF2XLuP6RKLnhMa2vfYduHTs32gddZWdj7fDeQgcpXEyUx1QVLoTkpqi0rAdJru75NcXIKUF0B3qc1e7IKMC86vqF6Bsme5qACUHTu0uPI7V4gzqe5lse0ibYstz1mDNg5Xtb5j1wNQfpn9zq1V7WKTbnnZb80exD+XqXw+1zmvXMOG7tl82cIixWhCKy5mRYVxJbudH4cyOEES2VNzl4tHdLv+vLwHgLRYCUU6eK4EhHXpWYVTcNO4C1+obIdrrT4/kQLIYlhEtccmejonyJQw8pMvQkKRKErgQIHgxIg9dcu8hej2LhKq8CFFiKy6iHXz2kHYXpySWtqS0pU2spiUannWUu76/M/2ezdxKEL2AEKULatKBWuIaQ4sUsudXDJDZFoOu42/PHQB+XjWkxMuK0i2wzLLdCdmt9VvRCKbloLsjKxG8KxlhezcAtCdlfSEKzpNA8yUHvCWe9stMFYYBBAXKt/a6uVaqrKuokuJrICWHdcfN+UPO3DFg2Uusca6/FxlCWm+d2ddIfvunNXkCZezxl2IKqNEk5mlAYvUMmRepVsqJ1Mgy+TyrCkrVk6oAtefdu+Zz7af7thHaAEFpaeuSET2ZC2yYwiibOAvkLn2DHoaDM/tZxMbTMdcFnvKLrf3No4ycbBWTORY7jsKLLxS4bH3clCi68eibBzKt5Z8SyqRXpKErXZufg8teIJlrgvIpiOx1rSdOyp07fV5edFZh43wZ3Uh2apqKixSyxU/aaIKoSXlP0HDuvfyrkHbqZHprGynpVpGrCRyrj99ILjUcy0mep1I4fw8ZNeZ7Hay7j4inemnVOb2c25IkQXM/c49kW6MlQ2MVxUqoIcg9LJefAEKvguy6etFx4t1qkpWnwMqYgV3PbxEvk8nUJ6Lj5LEq7NovvOWJ07GclIj+rNK4Cxu90Bivhr9TsbNay7VWcDVLi1HmHRSVEiXaSwsUkwxc7S4/ISJbBlcZp9vRbknadtHk32qFi5mRGRcc6ZMklDmid66CpWZIC90+/mf/RiOrUwRCownVKXXF1tf1tGVPaVb12O4jZC9M8QiY3/m4uLTu0cEyv/sv2wzXEwqe2DR30vg2vMtKgHn8vVFSo9x0+86LYKMy6/yJRSzAOPGmMHBIsXEic3Aa//vLwvGxOTGRhm3np3k0Ll5kkyslJu+wR7fvDuLSkCmWphESpAdM25KSvN/0klyCroytCJTfcBM2YBUixgR3DQOpjKFALQ1I61vMp+AUItLr5dA+e/ZyXu7n6Ljo5S2pnoRDjNwy4PUczvMwMaYpARaXmq5saDUSKKfH0YkVKLnxVKjJt3cWlTSWFICzu3n4lGAmX2YnNUk7IOJZ1HlsA8Z4a0vum2+FeW7+ngwb+NhkWK6CbO6iuJQwfJoAVnPLejHIfxXPkUZxodnOytAQWd3SRjhECIbwgKY8kOAb74JSLOzsa5S02ZbFsd3XboAvahsTTnmKlCLhS9QblnwQBIkyDiBEtl3qV1+mTvQWU05Nx+8BxLfeso+2+8aAEDmIcN6eYW2qGJ1DX26rK1wfFSwjBkuWKSYLO5gPxcQrRThWVHdy6ybz/vsiZPKJVBk72SqTliREhLGxWcObwf5mti9Iu0ak5BASiBJEMIM7E0JIk3zBUiNMOUK0lbJ8utjgGb3NgvQOVprKoYX8xK+CPnDDID87yB82RqM3mBcf7C28oYaUAIXf3KWVKIfJpTJ7MvFIgG4SiNE7pnDFUUSJg19zvcl/3/yLavsxszt+MxAYZFiHJUG8XpQ8MTtD+B1WX02YcKlHAdP1rbDsk/ZLd2xucA5Qbv1UtvPWted0E2yD9mkAxpCmvUJAR2CkKmzpnIDfbuuK55h13vMU43WUtRijbjvdMXVYFmB69AeV/tD8+fxBSrptqZy9RmtNeWKB8useHDSbT2pLkvKuvmsgNm2AXYKDt+i0lZQpluViLlA2XIaelikBkmsAzMdhV/OxJU7KevwBjU7b4zYQE4jDLkmEeWrmCvSzaR4PYCumVc9C8o+ZesBv2HfIqAEZUOblIBI7JSBgEig41SJjWMYf5GEiacgn/EXpKUDyKeqe2OT7JghcpUqwviRHJxbr+g7j53TikuRZeCVvCo+X0Sg7HL30GFSyxORDcz2kyNsqnmiH1bylhRy33l2XuTdfsg+Z2npJj7lPpO3PBOjwulNqiSRxLL97OeCsWoLWVWutHxSE13LNcEitdD4T7RuES3uVNDmj5hMSjeE0H+USZJ1AMImLCD/hGpcapAEITN3m8NaVX4pHM+KUi2TnjwaCBiQVbCwCRQdQM5mgiQ7gEjhxj7Jtk6kgIJOtkgJQEtP9dFRZup0eEkUIt+RJ35Hn+QKugobcBeUDXh1ty/y3YXp4r3ciV0JKj0eSmJClXTPHNjvmDg7JXv+uFLXZ5QSNKItKGpJXUHCJEmQFFAjAqqlBSod8dx+iRYoNSJyYiRcLUZze8xqYVPQjQUtOwSR6m1Fmr1kW9fn82s3dllPvlsvliQhzT1Tqfd9ZzX79CH6FKK6U9qXsABVgUVqMWiiUMUIKjZEl9maemZCOrsuZ2XZvj+wpGxl9CyGkbn4XOzCYHMghAKgBCglPbwJumMUZhoPBSNcxqASqTmJOQagTHwjsBR8rKAk5l1526RpPqYTCJa+RYHVZQWlSKjKBCr2m/CPGx4qVsqoygR/ve6HtaSsi68VS5Kwrj5AtaC/wxYyt1/LfCf+mCcyCef+84K1nqy7V1EmUDYhwoqRy/IL5hbzkyZ8rFUMZPc2Nv6sYF9m4WGRWiK4MjuDCv764mSfTsP+0FlX+s26aFyn48cdwl2NgNmOzIqX7xYSHd0R2llvRUc/ZSc2AUJogZKp+awAlQrTF0k9c7gdf2XmoSIrHL7LqGhKDCGi1gqAnAgIT+Rch+hSwyM1+PzsQiDrKMvGSlXFilZRHT5/wkobd3J1Gq1rzxus29IVJdSIdOnlylhNqqUfGJQVJJtMYR5AsuEJ6Pod2IeMzKUHV7cRCsZagrGEoe9LTHf7EZIK99ZVP3f7zNGqWebW0HxgkWKq47n+LMI8mep+J29h2SzwrniWJ1a5jgvmybtFuaQK/5jJrBYZkoBsA6ojdDafAgDtAvSzAEUqoaCMnupOX5jj6w1E5sozT+Bkyh+ITsQ9FGYx2k7RilduMK2KCJbvRuw1Fqrb4u6LorFudllQGSSXZi4laMyMifIqSahRI1It4d61WAGpdfdJa0FZ95+5HPt7ANz32vUgo/yX5+azbkFrJeUehAKxsVZU1cSJgu9hCGcxWpKwSA0ztrPr5SYMn9TtvtIM/vTdHtbFFbOa/P2tqyX2gE6kNci5dPzOqKSt1oKyT965MVR6QG8KcoM9yfarJCBSIE11qnIC0uEjISATAmCEympDIiAgQS1AQOnLtYpKpOeb8u9XOLYq1+YgYcGKUr9P3FUz+0oP4R0jJlC59Uag/IG6xrIiOwbKq8Nn3XrKc/E5C8p+X4n33dnhBfZ7NLgisX67c9aT/Z2Q+93kkycosNStpYrstxoSE6tY/Cnm6iuYFqWygJU8iFSdR2q5wyK1FFDUW6g8wsC/Xqif2nNVsJXXWSsAMJYBzJTu+mBeO2CqPWTxAP2uRUQkAIzIFI1JUS1COqqPo0ZIi1SLnItIpAIgIJ0REB1tOSVHtbuv1RJIZgmqLdA6Zp7GyVSmSCQoMUF2IzoilZBSD9wlG+9QSsd1yAwiVSbhwrea+ulczD4U21fI7h7bp0p9vfzK8nFPwbsuYZSYNPIky4Y0ApWuaJkhAVmauRrNXHzWakpHTSbfCEycynz20s4B+32Yn5pz53mv1LprtQUl2+aBxCZLeLEpmAkOAeMKtPc6fHlVJkgFIhQrL1VBoCrDLr5aYJEaFmK13cooSiOPdXLWguo6J2Wds28xVIl7WRefFVDTKbk/d/JcPz4COlVcQk8NL0mPeTJ1+Mi829RzCOE6N2oBylQtUImABEElgKTsqR4AKBV25o+sOK29NL2XKWhrrAlAC5W9Ll88XBJDQVJESJjG3uPhorSmXtexZbe1ZPfxhMsNyvYKxbr6e7k5oYxAtWQ+xdxYTy7e5FlPKkFmDds4o02IKcOzmmyJpMy9h27Rsdjj+lZUKFQ+FV2tWXX0btGarxtwuaaSzxUWqWGlSrzCn5Oo6jGVzGI6fsfrstyCP14/4QAw5YdMhyGM1aTIPS1LCNMsApSX8KBgx+PCDb4VxoJqEURL5SoOqHYCSgVUW3eiOotPIJk1ae2SzDKYVHcgacMtEylBmrFXOjiftRmpPr8fryJjcQl/6nl498PdI8rfc/u0799m//5WTXTpFV9y23ULU25Qro1FAdp6MpMT6ndo116iRSsdte4+L+40mr1ba8m6/ajliZN11dp3BEJlXXgK3pABm25OmUXVIchUucw+oUhbwXaaeHuPPYvJxaP8Wn05EYtYSKTKU877FaiiOBe7+PqGRWq5UfRH5qwm5TLRSJqO1LfKvAw4ksiyrCRlx0h1WSIStlOBrgSRAtrdZ4TCjpMRmaVlXy7XQmiBao2kEJKQJPqPPx1NQUqg006QJokWFpWAWkAitWDZcVQyAWQHsGO9QBJCEgClSykRAR1zYmmtNnjxJe0WE0qB0Mndw65MQJe2bi4iMsNuV+yoilAVCVFsvfl/LtbkJ0m4ySdlJk4jiWcpWZedjttZd59KjEglQDpqxMmmm3uxqdxkhtCfnZaHFrTn8pPW3dc2VlTHuPc6+uFAEPRviyg/JspauVaglNLLPIHKzRtVZF3Bd8t629VkQTFzg0VqqWDcalVT0YmMlSM9i8wXKlsZw2TudQmV2Z+k7jRIWRcSZRl/ZOM8cNsDIotTmTiEHs8EY1EJE3vQn7OOTgufFSkh7BQeQDsVQEdCjZGu9WY6PpsBaMsxAQSZCogRW2BCQpDKEkCMJSegn3hFInJJFQQJ0UpyLid/cHNesIyJ2CViQQp7ZUsqIkgF45q6qkUEKeW6Cry2nlRLx8XcmCcp8qnlnltPJZn1pKeDt5aUPa++j7446RXIxMmzoNz37P/fc/MJygbqZnOMUe7d/y6ikxuGAmUJLRpFeYFyyyMCVWYNFbhv2cU3d1iklhJVEyjCeJW1nqQXfwotKrtfGnSWSltawv1fu32QCghTSw+JhBSkwwdSu91Uot/J1NqjRFc2l/ZpnHQnaGNHeoiSwoqRDoSwrjrCbCfBsdERdDoJZuUoVFtCHpNQo8KMq9IuQNnWL5Hq2SZkRydZWFejlPYpnHSfQdngUUqMALUISGXOxekqWNjO0E999jtPIG7FVn06DwSpS4jCbUNhMqnkLqXcbKNG9JebjnqFYk16eWfMWkqmUGwCpCuMlTUKpCMw20OLuP/TUMKMZ/JEKPXeU23damvXTLnSIchZgmyb2ZfbylhSJklCIf9uhwjYBwTfgjLVJbLvxLeMQiFTcOOhesWgernr2J1XOyxSQ0C0zp9eYRd270NeBp//ZNjlfioRttCiiiRRCEW6fVbgALgqFO7J2WZlmeamwsSCjMVj3q1bSKS6Np9dBxLu0EIQEqmQGKHS8ZIUQgCd0VT3RyM6ViWF7kAzV5OupK1a1qqCcX15liAAYeJl1hMozGUIEiBTTslVNxAEMkkWgjxfpZcRSNYFGNY+7KdDC5MeRIlIWTcfkAmULQorMoEjb66nsO6e8ktWJXCDc12JIyNaOkmC8pcWXpbn6vOtqMxiysQrZ2V5693DgH13xy4ZE1VmQcWsl/kKVHi4XttXmWvMp6rVvcRgkWo44Q89Wj5pjoM+u1LRQ7HRC51FpRMijBVmO2HhPeWaYwkhTJqxgJQKZLLxpJD671IYN47QnaatEOEXJhXKPL2n+ik/NW6ydivRdftaHUhBaEkFjHQgpUJnTKIjgTQhpDKB6ujIvZoV2pIa1QJI1pprCbSMlZTMmoGjKUGm+ulbmioHNv1Zu0iNC8/eIpUldJDyBKwoZd27t9lsxD06szD25CdCAFk1eultI0Q2rYadUiPJi5PyShvZhAgrUCQFOiszobIZfOmYEasRQI1Qfgp483PxK9jLjnlvA8mstZo8S6qtEyTkrK3HpyDbJkmlnWaJEmmqj22tnVRlSSz+fbbiYh/M0jS/vqjs0XxcfBE4QaI+WKSWG7ZwbG4ZZbGp0LKy8RU7hsoew4tPiVTpp3OXOKHdf2RqxglI3ZEL0gkLbSNGbUIiAaWEfk8A2dJZ0Sl0B0YQUG0JNatnf+0YwWyZlPmWVJDGuuqkCaQktEULqQCoI9BRArJlO0mTrk4AtQQSqa0baS0h04GiA9gMQy2mAJERnw7pzENrSNlsRiJAJMaCpCzAD2jxKBOiKi4/f3yTEPnB1XaZPZcTKbhqEU6chKkaYQVpJKsWkZsDqqVde7ZihM3aU6MwcSjKplkR5h4YCxmAy9qzFpIWJHKfbSZf0tYPALKj3XqyYzMokROorIhsliTRlWbux5RiLj4vgy9/++u1oDjGVC8sUkOIfUrrVZC2NIkiOo7KEyofCTiLymWvmf2V7rRtenEukcB01m7ckTSJCKb0gxMFMq4jEpAdArVNNYm2TgQRs9oSUATMJCPotEyGX0u62FSqpDNurCBCCiAh7aoiAWUsRzUCZ60JRaBUO/SslaUnQ9RiikRkxU2VgLTV000wn+z9UiajkaCtKShdJsp1djpe131f7fdR+lVm25rvLCdUzqIy99hYW1akbOV5NSLdnF0u1tQKRMq58vR9cgVibVJEQl2TFeqkhiwRRqQiiz91MotJdJCram7TzZ1AeYN24aWauxgUvN+W7+LzY36xWZXt+lgWX7agt0DVMZVOz1JYbIGFsEgtR/y4Ute6iOtQkRMqm1HnLCqbog5oa0RBWxp2v0Tooq4wBofJmCMT6BFKutJFUmoXoFCkOzXScRKhBNJZPW6nnQq0E0J7ZQutkQ6kJLRaOtiljJUlJEG2CEoo0KhwA1MpsWnwpsZfC1Ajepma0Z2qbGuhFEq4igd2DI9QWkBh4ySEbOyOjbMom+0ovbqF3n33MTEie28Kt/NEyFX6EHDWkltvY0xOrJCllLeM607ACZPyM/Scuy8TpnTMPDyITJic9WTbYO8DjAi1TRWQGWMttbUYyTbQOmbupydSclaPd0raKks1bxvXXjvN3HtFSRL2dwgAaZolSqg0++0GFlSXQHmuvnlZUMxAYJFaarjBpH0GWX03n/+0ZwXLCE7UHRiLZVkhVAASwA4E1inn5GbQFUqXH5IpQUGYgZwmscJYWaIDiDYghQC1tdtRtQkdkeisPDMoVykBCpIsSFKWBGHKMlGiLStKtdUghHn6d7Em4ZI6rAEkhLYIVKItWLJp0bYGrfDeSWQuwNyDc3eCg3XTEbyEgC6Ly4s9ybww6ZRvATtxpIs7GVHR7jxbJSITJ1sENlcI1hOpLEZIrtlOqIJLEeYChB2cTdaighuY6ywr9yLPQoUe92T+b2NPImIpdSVJxNLGLVUEymNOFlQdsPVUCosUU45vWdlUN+VNkOisKW8fmXUkegCsNO4vCUjt4rPzB2kDTWejZRMY6o5UEJmp5AVk26Skm4oYaUcgHZFIW4R0TGf3QZApbySgUtubIoshSQCwHTIBsEka5rw2fmWe9ElmLiyZ6g5fdshZUK4z9jPV0iwrTd8zT3xit9cmmxBlSXEFbsFs8kjkrCUrWDbJwc10jEyQfHHSA3SRG+uUjsIJk70/btqUCG7Mk3XxKQE5m7n2khl9D5PZ7H4ms/o+6diUTkzJKkwQZEcBHeWSJFxsSQVJEi6tPC9QuXTzXrGnkLlOwcEMHBappUrVMVNVyAlVxB3o//GbrDZtIRmhIi0SOkYFHaMyiQpISOcVpPo4SdscRgGAyIrSpgKqo5dRCwAk1JgAjdjxV6TT1u0gX5VZVG4qEAkQyI3jUiNaPv0HZRuTkrY6RgpIY4noKhvCCI+9TpOdSMYaa1nBAmzcqpBcR5q5RHVD8qusWOQsJU+8rNWkTD1DO3+TL1JujicXk0JuQK61nKhlvHjG1QfPXYvgpd13wrxn49HsmDQ9WzKyeJONRdlKEh1dSUKasVCu3JEvRP7Lr6EYe/fpVeqIGQpYpBaLSMHYQaWt+n+gLonCBIrJS56IFgENi6kmAn6NP2dN5QYHB0+8UDq+ZILvAjoWI6QCOtqcklIZq0APNCVFXiUK6xo0HW5Hd8xCAaptagHaoL7M3FP6Sd/8x7ih9IV6ri0jNGQmQ9SuQLObEjrJQwJItLVAncyKsm5BSjPBEmQTLeyJPEsq/HrLpi2Bpw/WzSaENzeTZ1EJT5ASa3Hp5Xk3Xr7Gnhv/JJDN4VUQ/3cxOM+dJ1N9z3T8EN6gacpSy83ElC7N3GbyWZHyq893lHH7ZRZU4TgoovyAXSCfbu79fUUrSeS+h8jfXWxm5TKi467YOqsDFqmmUEfmUJXTGBcczHQU9g8dQmR9qBGcaFag+4NW2ioSplKFneLCZvwJk0psrQ8pIYTSLqokuyTbEYuOPk5q3EckdSdIQgfibfxEzuoONpmxE+4BaiyBSshMFUH5WIrf/xhR0INQSU9HL4RpgxkDZQRMGMESyrTJdNCiQ8aKghtkKo0g6fR2ymIzCCwq3+D0LabYbRbZchtT0p/hBuQ6QbEiZa0h40d1Mx178SfY7SS5sU/2GDmM+9RW5LDjn2TbxJ7aWSq5Hv+k3XmuqseMn1quhap1tGMy9qyr1Ljx/CSJVGXi5Lv4wrFmaQpqG7M7jB/FShxF0sxD7O89N728L1T+OdzHAguOqQ0WqSGhV7p5lDKXX1ghvSzjL3ZMX6xyEycq5/LTDReZ689sLqSu8KDfTcFVSabyhCmDk+gBuG6GXqM4tnySTlk3TZdAkupkBTLuOxXr/EPRMsvcpYvsHjgXmYmdwYipNIkZzoXpheN09r3Q00SZCt1CaOuKrKjB3u4Sq9lPKRdZO2wqea7KuGdp+TGpLkvK3kdncZoGh88/vngaMXbFf5VxadrkBxOrEx046zGbD4pcCrr7Tr05odycY2GSRFGKedH9ighUtq66QOVvv4gL1aBYppUkqsIitdSZb2wqtn8uE9CYE0q6WJOLHXiDTUWqdPq08aUJYV1nwm1nn2RFRy9TI8almEo38FRaK8u3rsaMy3BWOOuAWjYFm/LWgn8pQQcM65I0AqjMX4ebqty4FW0HHrrAlJ+arjxri/LHyRoTJyzQ6rvhfHdfkSWVi2HZ8U0SUGbySBtzipYy8sRUtvX/pRn3BKUrdVgXX1YtQt+DZNbU30uBZEYhm4JDJ0XIjhdvIpvJp7dDJ80sKJtu7sefApHqHue0QNYLW0kLDovUMiQ6M2+/28WECqYHjUztkftsBEwAWUwsJZ351zaDa70KFoIANSJNxpyZ74qQDRolO22IL1LW3ZXFZ8gMIg4Ho4bWlc0wJP/ykFlUbgJH++5ce75wGUvK9mlFD+NFrj5vXc59564FORED4KpD5KbJcPuFFSLy53Lthm2/HvOUKwyr4ATJxp2sm08oIGln7j0bb5LWeurkRcnFoVySRODi860nu97dIIpbN7EkiToTJligFgUWqaVCSbHZ0rFTvtvP+a+yZbkpPXz8yhRWUMLUdBufghY0e3b9FG+sJvs50RYWSWk6MKGNFimyqhQtAcCWYDJp4SmQmoQKkdo4iwjiL7YjF7kOPidWobXj922eaDgB8/pP4a93VpfIHy88pt0+5pK0mwdC49qO4Brs9t6kg9nxKMsE9ATJjibwRSl7F9lAXDfeyYqUyWhsw5Q0ykRKtsnNAZXMKidOevyTGaTrXHjIYlA5ccqWhe66vPh4rr0qSRIlbj7O/Gs2LFKMxk+mCKwnJ1RAfsCvP9AXgMmkAAkJkUidlOGOm/n2s5p2EqSksZa0FSaEykr+pIlOrOpICFODTo6Y93ZWf84WR3XZbRImFVuLlEqEc3lpF2DeDdY1/xECAfPFgCJaY58PPNdeoeWU2xGlQkWx8/v7yPx2NkMven7/HEoPZQtFyQ1gNg8AyVE/xoQuQbKZe1aYZFvPogs75klRlhihPJHy4k3CuvTSNMvoC6fZ8Aln1Y1ZNzzmaUnBIjUkxLKIoskUvSwqW73A/JHnq6CrbovK1oqzrr9elSnMI3rXVCH+9Oe+GeK7AGVmXZESOhuQjGD59ddIixnMgFXZ0dYToMdIkTS+OtOZS9smMwaKyLOuEnO+wDIRhJxYOFdfDH+d/dzbm5p32cW299cFrr385IK9FTE+dXtWIcJaq7mkiI6fDIGc60+aMVBCURZ3Sr3kCDtZoSsMS90xpnCogh0WQf53HREct723zmb0hW5BoD/RKkqSCNrBVc4Xjr5zmx955BFcdtllmJiYgBAC9913X249EeGWW27BqaeeipUrV2LLli149tlnc9u8/PLL2Lp1K1atWoXVq1fj6quvxqFDh+Z1IUuaufjCi/bp9ccVZkTF6prZAqD2Zc8XfKZU6SkTbKp7mgKdjg6Sm5cwL73OW56mOiOso5/ARTvVk+HNKsjZFMmMQutYiuSYQuuoQutIipEjCiOHlX43r9HDhNFDhJHDhJHDwMghQuuw/T+hdRhoHQleR4HWMV1/LveyFRWCl+243SDXEFH8otCSiq3zX+7L0NokjNg4a6gDr6irQDIrkMwIyBmB5KhAckSgdVigdUSY+2FerwCj08DoNOnXK+Z+HSaMHLEv5e5x63CK1pEUraMpkiMdJMdSyJkUyUwKOdOBmO1oy8l9f53sNdt2n7PvvAN0Oma6E/N7sVXM7Ss1rsDwNxxNA5+DQBXBsahFpW+ROnz4MM4991zceeed0fW33XYb7rjjDtx9993Yu3cvjj/+eFxyySU4duyY22br1q344Q9/iF27duGBBx7AI488gmuuuWbuV8H0R9WnwDKhKjseeU+55gmXcp1Nlslln56zMTMq65BMBQIXYLcZYu5FkLMp5KxCMqtMMF8PGk1m9Odk1r6Q+yxns7iKXS7NOB9pO/q21+FbQVLdr7AKQzSe5d+eIgES+fVdwpQ7SP5cWSVyYbLxRJYCbitBeBXJ7T3IRJjMC0iOmdesrgCixz+Z+zmj4036fivIWfOdtL2HilRbTqKjsgKx5nvUDyvKExy9jKwFZS0k38r2H4bc9We/L/e7LHnAYoYXQfOIGgohcO+99+Lyyy8HoH8oExMT+MhHPoL//J//MwBgamoKa9euxZe//GVceeWVePrpp3HWWWfh8ccfxwUXXAAAePDBB/H2t78dP//5zzExMdHzvNPT0xgfH8ebxeVoiZG5Nn+46DGgt3QcVWzfgu1z7j/fRRcuC/eRIjtPbmrzYHvvvEJKPbdVZCI/CD09h06q0IOESQjATn1uywPZKt9S5KelaNn1cOnadkJFvQ5ucj9/LJIbP4Rsma32EGbVWWIxrZirLrZ9V4WHMC4Wo+B4YdKHnuUYOTeejTe59PAOvEG6fpknMzkhWZefWTerxzSJtnLVIkTHJj9YMYInTsbt10mBdifv6vWm1SDnClTIiZNdFhIrGutP09HrwSpG2fkCSt19bHlVokNtfJfuw9TUFFatWlW4Xa0xqeeeew6Tk5PYsmWLWzY+Po5NmzZhz549uPLKK7Fnzx6sXr3aCRQAbNmyBVJK7N27F7/927/dddyZmRnMzMy4/09PT9fZbMYjGksqGwyZK4ekukUp/INN4bbRdc8N4USMQkCYqTeopYVMCAGkUr+bif5EYsVJv9tK4VLqWJZKTMq6FTIrVk687DQU2SDZLD6VxbUgyFVuiCc2eMrhuetEuC08jZHoFqIiQYusy29ohIiyz4B1RVJOpLK4Ejnr0MaXrCAB9ljewFylZ86F8orBUoEweVaNm1rDvvxsvVAYYsvKisX2iEENQqCYhaVWkZqcnAQArF27Nrd87dq1bt3k5CROOeWUfCNaLaxZs8ZtE7Jz50584hOfqLOpw0csSSG32nQsZckU/r7hH763X5dQuW0CwbLrkyRLqDB1/codyfaYplCen0yRa7e5JjsQ2LWbgETqHIUUuqis0O9IjFbCHppyKeNZ9p3Qae9kx2uRaUZWmNYKmLAiZJrqUrm9ZAYB6hItYT6bzPduQSLE71NE0LqspRjWBUlwt1jaqTC8dVa43ASEyr6Ts7Cyew031knPkqucCFlR8gfo5orCAkHlCL19YWJEmGLutSHuZq5JoMJzdi2es7OJqYGhyO676aabsGPHDvf/6elprF+/fhFbNMTErB0gn7WH/B94VLAsZiwVkD29QxqhKjy356oxtf9y/bcvXN5UFtp953VGQuhUd1NaiZR2CwqJLCuxoxtlXYJuGnVvUkBbNdyKS1ZaSHjuPsqNv9Jtt8rhuwWz7e263Lt/mTF3X2zboI+MCpURH2lLMTpLyk4dYi2pzKqyaeN6+gzllglzz5wl5Wfr2WrlfiFYL94ogv+79WkKchMXxsSlokDFfoMDFChm8alVpNatWwcA2L9/P0499VS3fP/+/Xj961/vtjlw4EBuv06ng5dfftntHzI2NoaxsbE6m7q8KRKqos19yyqGcQvm09SDczh3ju09bao7YAvSBifN3I2p0nEpaKvKiQqQlV+SAkj1tO0gobPEpOmYjatQx6S0kggby1LQrjwTrxLGGMxS1pGJl5mDKhOpTIhs0VffunKXUiRUwhP2YHn+XgTviAgV2cQOrzQTRYTJxJi0qOn0cdhCsOQNviX7br4LW63cJkME8zs5cbLfnb/eLguFoIJA5RiUQLGl1GhqFamNGzdi3bp12L17txOl6elp7N27Fx/4wAcAAJs3b8bBgwexb98+nH/++QCAhx56CEopbNq0qc7mLF1KxkKVuv16HSP2xxqMqwpxFdXT1IiHERMFZNPWRrDWlumlKfCF5aw35060bSdX889aR9ZtB9JxKSGD0wvhrCxKpBejMuexAubHopJsUkES+vw26SJuRRlrz3MJlsWUYkkVhVg99AQrFCrrusvWe4KksnVZgVfosU1Ki7+zmEyHL9JMALLxTjbupFzppyLLya4jf7lPTJy85dGEiBhzmf498vfBbr1m0rdIHTp0CD/96U/d/5977jk89dRTWLNmDTZs2IDrrrsOn/zkJ3H66adj48aNuPnmmzExMeEyAM8880y89a1vxfve9z7cfffdaLfbuPbaa3HllVdWyuxjqkGK5lY5vd/zkJn6AzBlkKx7rUc2ooIWKtuLpyn87EAiykom2Y4npcxqMu4/v0AtkTYdhM0IBLJO1boGhZdoYVx9TlhgBMwKkxXoJBMm2RKB8ORdhTAF8OJZgJHvQwBdsapwk5wwmYeQNFumtyHY6TS0iMNZQm5qDLONW0/ZpIN5IbLu2yARIubOA7ykAytUKu/a85MhcjcknrUXHZRb4GqelwXFDAV9i9QTTzyBt7zlLe7/Nla0bds2fPnLX8ZHP/pRHD58GNdccw0OHjyIiy66CA8++CBWrFjh9rnnnntw7bXX4uKLL4aUEldccQXuuOOOGi5nmVFWXaLfYxQdp8LTqItJ6f8gV/8vup9xDcK6Bq1CGBdhWLw2nFTRHZpyqesiVZll5bb12mGrWqQCkAQ7+SJAEImNfSktSnZuDi82JiRBQWYJETDWk9nXWXqeG88XJtEVXEJW67AHLjPPuc/QZV05S8h8du+BMLn9jPXkYk1W6Ow9s2nhRQLli5N9962eUKB6pJLrQ8QFKhQgEfzGKltQzNAxr3FSi8WyHCfVi4jA9G1J9St24Vw7tqyRca0Vn8dslySZGEVS0F2b/OMWHAt2wkXbjvD8vmtQyq5xWU5MpL+9yVL0t0u87Z0r0OxjXYaAJ2Lx70BXbhc9rSi9LfIWk28VAU4UfBHKLK7Q6rHHoEyUrDCoQGTs+auIE5DP3rOz5NqafED2XZellNvjAN0z7/r7SpH7PfQtUuGM06Tqc/dxEkYlFmWcFNMsiv7oCsWr6h9XURq7sYBys/9Gz6Mtk6zCekTsXKdjYldSdD1Nu2PBszCEMFOQe9aYsYZcHAvIfGzGfehciybRAiBXmd0XMj8Rwx5GSCNOSieOZIkSott6yt1GKhSx3HaewOQsIsATLE+o7H0JLSO7XHnHtNuF1chDwpiTvxzIi5vytvUp+D1GBSrWBn9/RfHMyKI4l6Xot1u0fRMYhB1R4XfXFFikliHzjldVdTMWufzseCultLWSEmAnTJQiS193YqWyJIiC6hlEBGGSN9w5lHm3bkHAdPKUCY8UZoCxJ1rW0vMEyi0PLCu7zImc/w4v3hQ+udt2V+wshC8AQCBUBVaTXWa3KYohAVk18li8KbSYgKiY5MoauQZIuGle+h3z1Isiy6dMbGK/3TqtqGHB+xtoOixSS4H5xKTmQ/gHX1X4/EoWaZaCRwCQJBCt8GnXExkEHaDfHL/T9Ts6a105AfPiRCoQFi9hAip/PS4hw9tWoMClJ719/HYU0LdYRawZPUV7QZq3f29Cd124HZC5/goy6ygUPCD7Pdh17j668QLVBSqaKCF6W0pV8eegCoWVaRQsUsPCYglRVQKBig4G9vEzAkPCDiM3waI7aLZdZOCwdQVF3Y7+4SOnd0kPhe4kCjIMg45XCDO1lnEb2vOUWAhVZkrWjfJcf3ZfuyzmqvPEp0ucom47b11YwsinTJzC9WabwgkJu/a1wxLKj8csD1ikmGahVDY1k+u4vQFPLpbkuwRtmQWvo7cZ0OaYIuKKcy7B0NoxSRVdA22FTtvrchH6673P7n92+3C7WNymCjGRMf8XXZ15IEQxMQq3tSJhkylyx4tZPSXWje/iK403RTL1hiluxAwMFimmHiJZfpZo+rDbz+t0rMXjVbDo2s+vEQhE4le5HbJO0lpWuTYXWHMKpbUHhXMd5s9HQaKG+5x62/n7BfGjyoRuu8i6rs/2/2XWVNdUFwVusDJx6pW5V2JNRS2nyPkrD1hnlgQsUkz9hCnHAYVllnzXXKwTs+tSyltZXZaKK00BPU86nAVGufWmjTFLK+aG7BFfElJU2i5nSc1DpPqeFywmEKE7zxeHIkuvzFoKqTDHU6HlFMaNupqxMAPWmcWFRYqZP33WAiwlNj0IgHCAMAFOSLrr36WZQLqpQfzjKS/bLyJeMjtXaawomjAhi9eFy0KR6hWX6pWqHRufVpSFB8StlTIrqur4plib+xGnoE3LLvOOycEiNSzU5Yu3cznV9IfvnmSr1AIsqLKeO16sWkUP0YqVYMoJl4zcO+cmzC3Mr0MkecK3EmPuyDQSHyu6BiDu7iyiIE07V/Ej7a6XWJqJFxwrJ1y9RClsV5FVGEuECM8Xa1cFWMCWPixSy406rR706XLxB9gWHS/mCox15D2e4HNdV4qIABgXn+3TnatOwomVFPlMQHsN/vbhOXOuPv+caeF+2bFVoeVWJbuusLvuI8Ehfq4e2Xhl2xUdM2wPwxTAIrUcGaRQFT0Jl430B/ofY5VrQIXOLmKdOQvES7BwVpef9h49XtE6T4jJs2qE5350y4pT5yudr8gaKtu3aoJDVWEq2rZG7G9rYFYTZww2GhYpZmHoJYwVrKz5nT+I3fjxLU/0chM3zpXYhI/RSy9xQ4YUzcUE9LZIIsLUc1Bt7DwxBixQPr7Fzm6+5QOLFLNw1GzBzb0dlHfLeanouYkbffoR0KpzdflV3P19w3tUcbBsKVVLErnjLYAIxArOVvl9kNL1ElmolgUsUk1nEJ2FX61h3sfqnYiRi1nFqklYqlpTPaYB6bldWSJG2fisqvRyfboDW5eid2x/cLJPmShVbFvlig/uuDVbSTWKCpcyWj6wSC1H6iwu2RTrqB+K4lphZqHZpmeJp67jV+2MI2JUNHdHbN4lt66kxFS4Tf4g/W3PMIsAixQzf8qmREDcyoomWohuUahKbrr5sHkxkemVMViQCj/o6de6x3xp5jpQNjhIfHm/WXnzga0fpk9YpJjBMBcLixTs3FF9n65iZzqnFHeLPxXIgIiN+ao8SLafcVZlbegnKaMfKkyJwRUkmBAWKWbZUViWCYi7AvM7D6ZRQNx6i527YNr00okm+xWoupMSKs7ZVKUuHydMLC9YpJjBUTI5YmFnZK2pQVI0yy8KKl4Mml4WUD/U4borKe46V/pKdBCytxCx23DZwCLFLCqLUiS0JIuwTLxqi894VSsqzyNVAWsh9t3OhZoSo2qdQhYgxoNFChhcmjej6VGFwn9qFrLG9PiSc/abUl67QMXo8xyxNs3LXcfiwPTLfP4uKu7LIjUo6kzzXqpE5wqSg7Ws5tIRu0rqhhrbR0SuKGwvq6pUKOuaVn2Q8N8DMwdYpJoO/2E3g5L4Wl+UJQTM11prqjhZ6hpP1/TrrJtl3gcM2ShMhmEYZjnBIsUwDMM0Fnb3MUy/8Dgdhlkw2JJiGIZhGguLFNM4Gl1RoKlB+6a2i2HmCbv7mGZhav41VagGPo5rHjT1njkaet+YZsOWFMP0QeOFoKmwQDFzhEWKYZYALJ7MUoVFimEYhmksHJNimkdTXUPDMgNxU+8fUG+dzGVeiWG5wCLFMFVpeFJHo8UJqL+QM9fHXBYMyaMhwzSEpgpBU9tlGeRkkcyShi0phumXpgsCwywh2JJiGIZhGguLFMMwDNNYWKQYhmGYxtK3SD3yyCO47LLLMDExASEE7rvvPreu3W7jxhtvxDnnnIPjjz8eExMT+P3f/3289NJLuWO8/PLL2Lp1K1atWoXVq1fj6quvxqFDh+Z9MQzDMMzSom+ROnz4MM4991zceeedXeuOHDmCJ598EjfffDOefPJJfOMb38AzzzyDd7zjHbnttm7dih/+8IfYtWsXHnjgATzyyCO45ppr5n4VDMMwSxWi5r4WAEHzmLNaCIF7770Xl19+eeE2jz/+ON74xjfi+eefx4YNG/D000/jrLPOwuOPP44LLrgAAPDggw/i7W9/O37+859jYmKi53mnp6cxPj6ON4vL0RIjc21+xqBuNo/hYBgN/43NnSWavt+hNr6Lb2JqagqrVq0q3G7gMampqSkIIbB69WoAwJ49e7B69WonUACwZcsWSCmxd+/e6DFmZmYwPT2dezEMM0QIMZgX0wwG+P0NVKSOHTuGG2+8Ee95z3ucUk5OTuKUU07JbddqtbBmzRpMTk5Gj7Nz506Mj4+71/r16wfZbIZhGKYhDEyk2u023v3ud4OIcNddd83rWDfddBOmpqbc64UXXqiplQzDMEyTGUjFCStQzz//PB566KGcv3HdunU4cOBAbvtOp4OXX34Z69atix5vbGwMY2Njg2gqwzAM02Bqt6SsQD377LP427/9W5x88sm59Zs3b8bBgwexb98+t+yhhx6CUgqbNm2quzkMwzDMENO3JXXo0CH89Kc/df9/7rnn8NRTT2HNmjU49dRT8Tu/8zt48skn8cADDyBNUxdnWrNmDUZHR3HmmWfirW99K973vvfh7rvvRrvdxrXXXosrr7yyUmYfwzAMs3zoOwX9u9/9Lt7ylrd0Ld+2bRv+23/7b9i4cWN0v+985zt485vfDEAP5r322mtx//33Q0qJK664AnfccQdOOOGESm3gFHSGYZYNw5CCPoe+rkNtfJfu65mCPq9xUosFixTDMMuGYeiiByhSXLuPYRiGaSwsUgzDMExjYZFiGIZhGguLFMMwDNNYWKQYhmGYxsIixTAMwzQWFimGYRimsbBIMQzDMI2FRYphGIZpLCxSDMMwTGMZyFQdCwYRgAaXDBmGciZ1UUcJKC5PxTBMAFtSDMMwTGMZbkuqbppsDTAMszxZ5p4AtqQYhmGYxsIixTAMwzQWFimGYRimsbBIMQzDMI2FRYphGIZpLCxSDMMwTGNhkWIYhmEaC4sUwzAM01hYpBiGYZjGwiLFMAzDNBYWKYZhGKaxsEgxDMMwjYVFimEYhmksLFIMwzBMY2GRYhiGYRoLixTDMAzTWHjSQ6YeeLLH/hnEPWvqBHlNv9amt6/JzPXeVdyPLSmGWQxY1OdPXfeQv4u5swD3jkWqbpbL0xPDMMwCwO6+QbCchGpQT1LL6R4yDFMIW1IMwzBMY2GRYhiGYRoLixTDMAzTWFikGIZhmMbCIsUwDMM0lr5F6pFHHsFll12GiYkJCCFw3333FW77/ve/H0II3H777bnlL7/8MrZu3YpVq1Zh9erVuPrqq3Ho0KF+m8IwDMMscfoWqcOHD+Pcc8/FnXfeWbrdvffei0cffRQTExNd67Zu3Yof/vCH2LVrFx544AE88sgjuOaaa/ptCsMwDLPE6Xuc1Nve9ja87W1vK93mxRdfxAc/+EF8+9vfxqWXXppb9/TTT+PBBx/E448/jgsuuAAA8PnPfx5vf/vb8T//5/+MihrDMAyzPKk9JqWUwlVXXYUbbrgBZ599dtf6PXv2YPXq1U6gAGDLli2QUmLv3r3RY87MzGB6ejr3YhiGYZY+tYvUrbfeilarhQ996EPR9ZOTkzjllFNyy1qtFtasWYPJycnoPjt37sT4+Lh7rV+/vu5mMwzDMA2kVpHat28f/uRP/gRf/vKXIWosa3PTTTdhamrKvV544YXajs0wDMM0l1pF6u/+7u9w4MABbNiwAa1WC61WC88//zw+8pGP4LTTTgMArFu3DgcOHMjt1+l08PLLL2PdunXR446NjWHVqlW5F8MwDLP0qbXA7FVXXYUtW7bkll1yySW46qqr8N73vhcAsHnzZhw8eBD79u3D+eefDwB46KGHoJTCpk2b6mwOwzAMM+T0LVKHDh3CT3/6U/f/5557Dk899RTWrFmDDRs24OSTT85tPzIygnXr1uHXfu3XAABnnnkm3vrWt+J973sf7r77brTbbVx77bW48sorObOPYRiGydG3u++JJ57Aeeedh/POOw8AsGPHDpx33nm45ZZbKh/jnnvuwRlnnIGLL74Yb3/723HRRRfhC1/4Qr9NYRiGYZY4gmj4pqWcnp7G+Pg43ox3oiVG6jswz2HUPzyf1NxYbvetydfb5LY1nXncuw618V18E1NTU6V5Bjzp4SAYxI9+Ofzg64Y7H2YpsMz7Ey4wWzfDZ5g2jyH6A5ozy+EameYyRP3UcFtSQvAf+2LD93/uLKd7t5yudTkxr+9VABW0ki0phmEYprGwSDEMwzCNhUWKYRiGaSwsUgzDMExjYZFiGIZhGguLFMMwDNNYWKTqhlNtGYZhamO4x0k1FRYqhmGYWmBLimEYhmksLFIMwzBMY2GRYhiGYRoLixTDMAzTWIYyccJOgdWh9iK3pAeihmcAUvM/xnJlYJWeOTFmSdH030nT2zdHbP/da0rDoRSpV155BQDwPfzfSlV0F40mt42ZO/y9MlVo+u+kIe175ZVXMD4+Xrh+KGfmVUrhmWeewVlnnYUXXnihdFbHpjM9PY3169cP9XXwNTSDpXANwNK4Dr6G3hARXnnlFUxMTEDKYq/TUFpSUkq8+tWvBgCsWrVqaH8EPkvhOvgamsFSuAZgaVwHX0M5ZRaUhRMnGIZhmMbCIsUwDMM0lqEVqbGxMXz84x/H2NjYYjdlXiyF6+BraAZL4RqApXEdfA31MZSJEwzDMMzyYGgtKYZhGGbpwyLFMAzDNBYWKYZhGKaxsEgxDMMwjYVFimEYhmksQytSd955J0477TSsWLECmzZtwmOPPbbYTSpk586deMMb3oATTzwRp5xyCi6//HI888wzuW2OHTuG7du34+STT8YJJ5yAK664Avv371+kFvfmM5/5DIQQuO6669yyYbiGF198Eb/3e7+Hk08+GStXrsQ555yDJ554wq0nItxyyy049dRTsXLlSmzZsgXPPvvsIra4mzRNcfPNN2Pjxo1YuXIlfvVXfxV//Md/nCvU2bTreOSRR3DZZZdhYmICQgjcd999ufVV2vvyyy9j69atWLVqFVavXo2rr74ahw4dasQ1tNtt3HjjjTjnnHNw/PHHY2JiAr//+7+Pl156qVHX0Os6Qt7//vdDCIHbb789t3whr2MoReqv/uqvsGPHDnz84x/Hk08+iXPPPReXXHIJDhw4sNhNi/Lwww9j+/btePTRR7Fr1y6022381m/9Fg4fPuy2uf7663H//ffj61//Oh5++GG89NJLeNe73rWIrS7m8ccfx5//+Z/j13/913PLm34Nv/zlL3HhhRdiZGQE3/rWt/CjH/0I/+t//S+cdNJJbpvbbrsNd9xxB+6++27s3bsXxx9/PC655BIcO3ZsEVue59Zbb8Vdd92FP/3TP8XTTz+NW2+9Fbfddhs+//nPu22adh2HDx/GueeeizvvvDO6vkp7t27dih/+8IfYtWsXHnjgATzyyCO45pprFuoSSq/hyJEjePLJJ3HzzTfjySefxDe+8Q0888wzeMc73pHbbrGvAej9XVjuvfdePProo5iYmOhat6DXQUPIG9/4Rtq+fbv7f5qmNDExQTt37lzEVlXnwIEDBIAefvhhIiI6ePAgjYyM0Ne//nW3zdNPP00AaM+ePYvVzCivvPIKnX766bRr1y76d//u39GHP/xhIhqOa7jxxhvpoosuKlyvlKJ169bR//gf/8MtO3jwII2NjdFf/uVfLkQTK3HppZfSH/zBH+SWvetd76KtW7cSUfOvAwDde++97v9V2vujH/2IANDjjz/utvnWt75FQgh68cUXF6ztlvAaYjz22GMEgJ5//nkiat41EBVfx89//nN69atfTT/4wQ/ota99LX3uc59z6xb6OobOkpqdncW+ffuwZcsWt0xKiS1btmDPnj2L2LLqTE1NAQDWrFkDANi3bx/a7Xbums444wxs2LChcde0fft2XHrppbm2AsNxDX/zN3+DCy64AL/7u7+LU045Beeddx6++MUvuvXPPfccJicnc9cwPj6OTZs2NeYaAOBNb3oTdu/ejZ/85CcAgH/4h3/A9773PbztbW8DMDzXYanS3j179mD16tW44IIL3DZbtmyBlBJ79+5d8DZXYWpqCkIIrF69GsDwXINSCldddRVuuOEGnH322V3rF/o6hq4K+i9+8QukaYq1a9fmlq9duxY//vGPF6lV1VFK4brrrsOFF16I173udQCAyclJjI6Ouh+zZe3atZicnFyEVsb52te+hieffBKPP/5417phuIZ//ud/xl133YUdO3bgv/yX/4LHH38cH/rQhzA6Oopt27a5dsZ+W025BgD42Mc+hunpaZxxxhlIkgRpmuJTn/oUtm7dCgBDcx2WKu2dnJzEKaecklvfarWwZs2aRl7TsWPHcOONN+I973mPqyA+LNdw6623otVq4UMf+lB0/UJfx9CJ1LCzfft2/OAHP8D3vve9xW5KX7zwwgv48Ic/jF27dmHFihWL3Zw5oZTCBRdcgE9/+tMAgPPOOw8/+MEPcPfdd2Pbtm2L3Lrq/PVf/zXuuecefPWrX8XZZ5+Np556Ctdddx0mJiaG6jqWKu12G+9+97tBRLjrrrsWuzl9sW/fPvzJn/wJnnzySQjRjBmoh87d96pXvQpJknRlje3fvx/r1q1bpFZV49prr8UDDzyA73znO3jNa17jlq9btw6zs7M4ePBgbvsmXdO+fftw4MAB/MZv/AZarRZarRYefvhh3HHHHWi1Wli7dm3jr+HUU0/FWWedlVt25pln4mc/+xkAuHY2/bd1ww034GMf+xiuvPJKnHPOObjqqqtw/fXXY+fOnQCG5zosVdq7bt26rsSoTqeDl19+uVHXZAXq+eefx65du3LzMA3DNfzd3/0dDhw4gA0bNri/8+effx4f+chHcNpppwFY+OsYOpEaHR3F+eefj927d7tlSins3r0bmzdvXsSWFUNEuPbaa3HvvffioYcewsaNG3Przz//fIyMjOSu6ZlnnsHPfvazxlzTxRdfjH/6p3/CU0895V4XXHABtm7d6j43/RouvPDCrtT/n/zkJ3jta18LANi4cSPWrVuXu4bp6Wns3bu3MdcA6EyycCbTJEmglAIwPNdhqdLezZs34+DBg9i3b5/b5qGHHoJSCps2bVrwNsewAvXss8/ib//2b3HyySfn1g/DNVx11VX4x3/8x9zf+cTEBG644QZ8+9vfBrAI11F7KsYC8LWvfY3Gxsboy1/+Mv3oRz+ia665hlavXk2Tk5OL3bQoH/jAB2h8fJy++93v0r/8y7+415EjR9w273//+2nDhg300EMP0RNPPEGbN2+mzZs3L2Kre+Nn9xE1/xoee+wxarVa9KlPfYqeffZZuueee+i4446j//N//o/b5jOf+QytXr2avvnNb9I//uM/0jvf+U7auHEjHT16dBFbnmfbtm306le/mh544AF67rnn6Bvf+Aa96lWvoo9+9KNum6ZdxyuvvELf//736fvf/z4BoM9+9rP0/e9/32W+VWnvW9/6VjrvvPNo79699L3vfY9OP/10es973tOIa5idnaV3vOMd9JrXvIaeeuqp3N/5zMxMY66h13XECLP7iBb2OoZSpIiIPv/5z9OGDRtodHSU3vjGN9Kjjz662E0qBED09aUvfcltc/ToUfrDP/xDOumkk+i4446j3/7t36Z/+Zd/WbxGVyAUqWG4hvvvv59e97rX0djYGJ1xxhn0hS98IbdeKUU333wzrV27lsbGxujiiy+mZ555ZpFaG2d6epo+/OEP04YNG2jFihX0b/7Nv6H/+l//a64zbNp1fOc734n+DWzbtq1ye//f//t/9J73vIdOOOEEWrVqFb33ve+lV155pRHX8NxzzxX+nX/nO99pzDX0uo4YMZFayOvg+aQYhmGYxjJ0MSmGYRhm+cAixTAMwzQWFimGYRimsbBIMQzDMI2FRYphGIZpLCxSDMMwTGNhkWIYhmEaC4sUwzAM01hYpBiGYZjGwiLFMAzDNBYWKYZhGKax/P8JpxRID+uoEAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(X_list[5000][0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVb3lilkfeV9"
      },
      "outputs": [],
      "source": [
        "X_list = np.array(X_list)\n",
        "y_list = np.array(y_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvhunxywO2qt"
      },
      "outputs": [],
      "source": [
        "shuffle_list = np.arange(0, 10000, 1)\n",
        "random.shuffle( shuffle_list )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPgXfQMffAyZ"
      },
      "outputs": [],
      "source": [
        "X_list = X_list[shuffle_list]\n",
        "y_list = y_list[shuffle_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shqK8g-vzoqp",
        "outputId": "85b36f32-3813-4bf0-b9ed-578fbd6b5191"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10000, 1, 150, 150)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_list.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdVsYIQeOiGK"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, train = True, transform = None, target_transform = None):\n",
        "        global X_list, y_list\n",
        "        self.train = train\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.train:\n",
        "            return 9000\n",
        "        else:\n",
        "            return 1000\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        if self.train:\n",
        "            image, label = torch.from_numpy( X_list[idx] ), y_list[idx]\n",
        "        else:\n",
        "            image, label = torch.from_numpy( X_list[idx+9000] ), y_list[idx+9000]\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform( image )\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform( label )\n",
        "\n",
        "        return image, label "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOUvNkjGW7dr"
      },
      "source": [
        "### Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNkmSxzqucy0"
      },
      "outputs": [],
      "source": [
        "my_transform_augmentation = transforms.RandomRotation(degrees=90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qsp6pKV9n8sZ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aq4-0eXJn7t6"
      },
      "outputs": [],
      "source": [
        "train_set = MyDataset(train=True)\n",
        "train_loader =  DataLoader(train_set, batch_size=100, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4SBgLFxn-Kp"
      },
      "outputs": [],
      "source": [
        "train_set_not_aug = MyDataset(train=True)\n",
        "train_loader_not_aug =  DataLoader(train_set_not_aug, batch_size=100, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fN-R9dOzn7t7"
      },
      "outputs": [],
      "source": [
        "test_set = MyDataset(train=False,)\n",
        "test_loader =  DataLoader(test_set, batch_size=100, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRy4TM4vZyqE"
      },
      "source": [
        "## III. Train and Test Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0l4gzRkGrfnK"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwNKVkC2qk71"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    model.train()\n",
        "    \n",
        "    size = len(dataloader.dataset)\n",
        "    running_loss = 0.\n",
        "    #total_sample = 0\n",
        "    for batch_i, data in enumerate(dataloader):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        \n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Compute prediction and loss\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        \n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        num_batch_print = 9\n",
        "        if batch_i % num_batch_print == num_batch_print - 1:\n",
        "            current = (batch_i + 1) * len(inputs)\n",
        "            print(f'batch: [{batch_i + 1:5d}], loss: {running_loss / num_batch_print:.3f}')\n",
        "            print(f'temporary loss: {loss.item():>7f} | [{current:>5d}/{size:>5d}]')\n",
        "            running_loss = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2qgnoTkqr41"
      },
      "outputs": [],
      "source": [
        "def test_loop_Entropy(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    #size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    \n",
        "    test_loss, correct = 0, 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            # get the inputs for test dataset\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            \n",
        "            # calculate the outputs\n",
        "            outputs = model(images)\n",
        "            \n",
        "            # classify which class the output in\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            #_, labels_value = torch.max(labels.data, 1)\n",
        "            \n",
        "            # obtain the statistics of test loss and correctness\n",
        "            test_loss += loss_fn(outputs, labels).item()\n",
        "            correct += (predicted == labels).sum().item() \n",
        "            #correct += (predicted == labels_value).sum().item() \n",
        "            total += labels.size(0)\n",
        "\n",
        "    print(f\"Test Error: \\n Accuracy: {(100 * correct / total):>0.1f}%\")\n",
        "    print(f\"Avg loss: {test_loss / num_batches:>8f} \\n\")\n",
        "    \n",
        "    return test_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdlmKkOt6rrt"
      },
      "outputs": [],
      "source": [
        "def test_loop_prob(dataloader, model, class_i):\n",
        "    model.eval()\n",
        "    #size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    \n",
        "    total_prob = list()\n",
        "    total_label = list()\n",
        "    i = 0\n",
        "    with torch.no_grad():\n",
        "        for (i, data) in enumerate(dataloader):\n",
        "            # get the inputs for test dataset\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            \n",
        "            # calculate the outputs\n",
        "            outputs = model(images)\n",
        "            probabilities = F.softmax(outputs, dim=1)[:, class_i]\n",
        "            \n",
        "            total_prob += torch.Tensor.cpu(probabilities).numpy().tolist()\n",
        "            \n",
        "            total_label += torch.Tensor.cpu(labels==class_i).numpy().tolist()\n",
        "            \n",
        "            if i % 25 == 0:\n",
        "                print(f\"Process: {i/num_batches*100}\\%\")\n",
        "    return np.array(total_prob), np.array(total_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlUgDkGmnc8N"
      },
      "source": [
        "## III. ViT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MIhaP4t28X2"
      },
      "source": [
        "The implementation is from https://github.com/lucidrains/vit-pytorch, where we change the prenorm to postnorm.\n",
        "\n",
        "Batch size: 500\n",
        "\n",
        "Training on: A100 GPU\n",
        "\n",
        "Learning Rate: if the test loss isn't improved for three consecutive epochs, the learning rate will be reduced by a factor of ten.\n",
        "\n",
        "Optimizer: Adam\n",
        "\n",
        "Test Accuracy: 50% (Pre-norm), 72.4% (Post-norm)\n",
        "\n",
        "ROC-AUC score: \n",
        "\n",
        "1. The Pre-norm ViT learn nothing and is alomost a coin tosser.\n",
        "2. However, the post-norm ViT can learn something, although the performance is worse than pre-trained ViT from pytorch libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PO_zbykLGED4"
      },
      "outputs": [],
      "source": [
        "def pair(t):\n",
        "    return t if isinstance(t, tuple) else (t, t)\n",
        "\n",
        "class PostNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.norm( self.fn(x, **kwargs) )\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head *  heads\n",
        "        project_out = not (heads == 1 and dim_head == dim)\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.attend = nn.Softmax(dim = -1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        ) if project_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
        "\n",
        "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
        "\n",
        "        attn = self.attend(dots)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        return self.to_out(out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbmExHfQNHee"
      },
      "source": [
        "### Pre-norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIpGbA-xOjfB"
      },
      "outputs": [],
      "source": [
        "class Transformer_pre(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
        "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
        "            ]))\n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x\n",
        "            x = ff(x) + x\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajqaXb7-Nfye"
      },
      "outputs": [],
      "source": [
        "class ViT_pre(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
        "        super().__init__()\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
        "        patch_dim = channels * patch_height * patch_width\n",
        "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
        "            nn.LayerNorm(patch_dim),\n",
        "            nn.Linear(patch_dim, dim),\n",
        "            nn.LayerNorm(dim),\n",
        "        )\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "        self.transformer = Transformer_pre(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
        "\n",
        "        self.pool = pool\n",
        "        self.to_latent = nn.Identity()\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.to_patch_embedding(img)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
        "\n",
        "        x = self.to_latent(x)\n",
        "        return self.mlp_head(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRgPTV02NOQq"
      },
      "source": [
        "### Post-norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ILMrCrUMO8e2"
      },
      "outputs": [],
      "source": [
        "class Transformer_post(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PostNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
        "                PostNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
        "            ]))\n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x\n",
        "            x = ff(x) + x\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "zvwS12i9MzB1"
      },
      "outputs": [],
      "source": [
        "class ViT_post(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
        "        super().__init__()\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
        "        patch_dim = channels * patch_height * patch_width\n",
        "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
        "            nn.LayerNorm(patch_dim),\n",
        "            nn.Linear(patch_dim, dim),\n",
        "            nn.LayerNorm(dim),\n",
        "        )\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "        self.transformer = Transformer_post(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
        "\n",
        "        self.pool = pool\n",
        "        self.to_latent = nn.Identity()\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.to_patch_embedding(img)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
        "\n",
        "        x = self.to_latent(x)\n",
        "        return self.mlp_head(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OegdOug928X2"
      },
      "source": [
        "### Training and Accuracy (ViT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19XzPHMo0GQA"
      },
      "source": [
        "Pre-norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "on7jKmBoQJlX"
      },
      "outputs": [],
      "source": [
        "model_ViT = ViT_pre(image_size=150, patch_size=6, num_classes=2, dim=32, depth=6, heads=16, mlp_dim=128, pool=\"cls\", channels=1, dim_head=16).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z60ow5tRQJlY",
        "outputId": "dccc01d1-53ae-4c1b-c87c-67b6823e3851"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------Epoch 1-------------\n",
            "current learning rate 1e-06\n",
            "batch: [    9], loss: 0.821\n",
            "temporary loss: 0.787494 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.848\n",
            "temporary loss: 0.821138 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.810\n",
            "temporary loss: 0.797683 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.812\n",
            "temporary loss: 0.787200 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.810\n",
            "temporary loss: 0.787695 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.818\n",
            "temporary loss: 0.870380 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.796\n",
            "temporary loss: 0.763347 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.784\n",
            "temporary loss: 0.814376 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.782\n",
            "temporary loss: 0.818362 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.819\n",
            "temporary loss: 0.786538 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 50.0%\n",
            "Avg loss: 0.789080 \n",
            "\n",
            "-------------Epoch 2-------------\n",
            "current learning rate 1e-06\n",
            "batch: [    9], loss: 0.808\n",
            "temporary loss: 0.829982 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.793\n",
            "temporary loss: 0.726696 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.780\n",
            "temporary loss: 0.816068 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.775\n",
            "temporary loss: 0.741078 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.759\n",
            "temporary loss: 0.797206 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.760\n",
            "temporary loss: 0.827604 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.791\n",
            "temporary loss: 0.767523 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.738\n",
            "temporary loss: 0.790119 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.749\n",
            "temporary loss: 0.737602 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.770\n",
            "temporary loss: 0.749364 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 50.0%\n",
            "Avg loss: 0.756299 \n",
            "\n",
            "-------------Epoch 3-------------\n",
            "current learning rate 1e-06\n",
            "batch: [    9], loss: 0.759\n",
            "temporary loss: 0.735071 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.765\n",
            "temporary loss: 0.796089 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.755\n",
            "temporary loss: 0.769874 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.744\n",
            "temporary loss: 0.795240 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.733\n",
            "temporary loss: 0.750604 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.748\n",
            "temporary loss: 0.736106 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.742\n",
            "temporary loss: 0.750059 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.737\n",
            "temporary loss: 0.723808 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.737\n",
            "temporary loss: 0.769174 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.722\n",
            "temporary loss: 0.699882 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 50.0%\n",
            "Avg loss: 0.732126 \n",
            "\n",
            "-------------Epoch 4-------------\n",
            "current learning rate 1e-06\n",
            "batch: [    9], loss: 0.722\n",
            "temporary loss: 0.712582 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.724\n",
            "temporary loss: 0.675658 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.724\n",
            "temporary loss: 0.724607 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.736\n",
            "temporary loss: 0.705963 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.737\n",
            "temporary loss: 0.711628 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.721\n",
            "temporary loss: 0.719443 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.711\n",
            "temporary loss: 0.740345 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.727\n",
            "temporary loss: 0.736942 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.733\n",
            "temporary loss: 0.747757 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.708\n",
            "temporary loss: 0.693798 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 50.0%\n",
            "Avg loss: 0.716136 \n",
            "\n",
            "-------------Epoch 5-------------\n",
            "current learning rate 1e-06\n",
            "batch: [    9], loss: 0.714\n",
            "temporary loss: 0.694493 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.726\n",
            "temporary loss: 0.725286 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.714\n",
            "temporary loss: 0.709842 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.705\n",
            "temporary loss: 0.707694 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.711\n",
            "temporary loss: 0.697308 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.714\n",
            "temporary loss: 0.719092 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.710\n",
            "temporary loss: 0.743539 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.710\n",
            "temporary loss: 0.696725 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.705\n",
            "temporary loss: 0.712197 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.705\n",
            "temporary loss: 0.683904 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 50.0%\n",
            "Avg loss: 0.706129 \n",
            "\n",
            "-------------Epoch 6-------------\n",
            "current learning rate 1e-06\n",
            "batch: [    9], loss: 0.701\n",
            "temporary loss: 0.676985 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.699\n",
            "temporary loss: 0.709261 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.716\n",
            "temporary loss: 0.721981 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.705\n",
            "temporary loss: 0.708431 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.715\n",
            "temporary loss: 0.717447 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.693\n",
            "temporary loss: 0.671179 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.703\n",
            "temporary loss: 0.719573 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.702\n",
            "temporary loss: 0.714592 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.700\n",
            "temporary loss: 0.690624 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.704\n",
            "temporary loss: 0.740807 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 50.0%\n",
            "Avg loss: 0.700747 \n",
            "\n",
            "-------------Epoch 7-------------\n",
            "current learning rate 1e-06\n",
            "batch: [    9], loss: 0.692\n",
            "temporary loss: 0.706351 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.710\n",
            "temporary loss: 0.694823 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.708\n",
            "temporary loss: 0.696645 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.698\n",
            "temporary loss: 0.691518 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.692\n",
            "temporary loss: 0.688455 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.703\n",
            "temporary loss: 0.716230 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.702\n",
            "temporary loss: 0.709518 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.695\n",
            "temporary loss: 0.692021 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.700\n",
            "temporary loss: 0.684031 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.697\n",
            "temporary loss: 0.681512 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 50.0%\n",
            "Avg loss: 0.697817 \n",
            "\n",
            "-------------Epoch 8-------------\n",
            "current learning rate 1e-06\n",
            "batch: [    9], loss: 0.696\n",
            "temporary loss: 0.697079 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.698\n",
            "temporary loss: 0.688454 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.698\n",
            "temporary loss: 0.697325 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.696\n",
            "temporary loss: 0.699270 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.697\n",
            "temporary loss: 0.696180 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.695\n",
            "temporary loss: 0.694731 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.698\n",
            "temporary loss: 0.691779 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.701\n",
            "temporary loss: 0.708489 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.700\n",
            "temporary loss: 0.697194 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.697\n",
            "temporary loss: 0.702610 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 49.7%\n",
            "Avg loss: 0.696493 \n",
            "\n",
            "-------------Epoch 9-------------\n",
            "current learning rate 1e-06\n",
            "batch: [    9], loss: 0.697\n",
            "temporary loss: 0.693227 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.696\n",
            "temporary loss: 0.708535 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.694\n",
            "temporary loss: 0.705329 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.697\n",
            "temporary loss: 0.690107 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.699\n",
            "temporary loss: 0.695282 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.697\n",
            "temporary loss: 0.691349 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.697\n",
            "temporary loss: 0.694919 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.696\n",
            "temporary loss: 0.686788 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.700\n",
            "temporary loss: 0.701477 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.694\n",
            "temporary loss: 0.691610 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 45.3%\n",
            "Avg loss: 0.695865 \n",
            "\n",
            "-------------Epoch 10-------------\n",
            "current learning rate 1e-06\n",
            "batch: [    9], loss: 0.695\n",
            "temporary loss: 0.696015 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.694\n",
            "temporary loss: 0.691044 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.699\n",
            "temporary loss: 0.698411 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.698\n",
            "temporary loss: 0.701505 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.700\n",
            "temporary loss: 0.709882 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.696\n",
            "temporary loss: 0.695750 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.694\n",
            "temporary loss: 0.689328 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.696\n",
            "temporary loss: 0.692905 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.695\n",
            "temporary loss: 0.692002 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.696\n",
            "temporary loss: 0.698275 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 48.3%\n",
            "Avg loss: 0.695607 \n",
            "\n",
            "-------------Epoch 11-------------\n",
            "current learning rate 1e-07\n",
            "batch: [    9], loss: 0.695\n",
            "temporary loss: 0.699717 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.696\n",
            "temporary loss: 0.693682 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.696\n",
            "temporary loss: 0.698617 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.694\n",
            "temporary loss: 0.691576 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.697\n",
            "temporary loss: 0.701296 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.696\n",
            "temporary loss: 0.698561 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.695\n",
            "temporary loss: 0.692324 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.698\n",
            "temporary loss: 0.698998 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.697\n",
            "temporary loss: 0.698918 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.697\n",
            "temporary loss: 0.694421 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 48.3%\n",
            "Avg loss: 0.695572 \n",
            "\n",
            "-------------Epoch 12-------------\n",
            "current learning rate 1e-07\n",
            "batch: [    9], loss: 0.697\n",
            "temporary loss: 0.697335 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.696\n",
            "temporary loss: 0.695910 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.696\n",
            "temporary loss: 0.696182 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.695\n",
            "temporary loss: 0.689970 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.697\n",
            "temporary loss: 0.694689 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.697\n",
            "temporary loss: 0.691149 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.694\n",
            "temporary loss: 0.701235 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.695\n",
            "temporary loss: 0.696720 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.698\n",
            "temporary loss: 0.703574 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.694\n",
            "temporary loss: 0.699033 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 48.3%\n",
            "Avg loss: 0.695545 \n",
            "\n",
            "-------------Epoch 13-------------\n",
            "current learning rate 1e-07\n",
            "batch: [    9], loss: 0.697\n",
            "temporary loss: 0.690181 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.699\n",
            "temporary loss: 0.701776 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.696\n",
            "temporary loss: 0.698325 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.698\n",
            "temporary loss: 0.705140 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.697\n",
            "temporary loss: 0.698464 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.699\n",
            "temporary loss: 0.702734 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.695\n",
            "temporary loss: 0.686254 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.694\n",
            "temporary loss: 0.687492 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.692\n",
            "temporary loss: 0.695132 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.694\n",
            "temporary loss: 0.687177 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 47.8%\n",
            "Avg loss: 0.695516 \n",
            "\n",
            "-------------Epoch 14-------------\n",
            "current learning rate 1e-08\n",
            "batch: [    9], loss: 0.699\n",
            "temporary loss: 0.696250 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.697\n",
            "temporary loss: 0.694657 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.696\n",
            "temporary loss: 0.690001 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.693\n",
            "temporary loss: 0.693984 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.694\n",
            "temporary loss: 0.691349 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.698\n",
            "temporary loss: 0.694530 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.696\n",
            "temporary loss: 0.690357 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.693\n",
            "temporary loss: 0.687557 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.696\n",
            "temporary loss: 0.696422 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.699\n",
            "temporary loss: 0.705013 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 47.8%\n",
            "Avg loss: 0.695515 \n",
            "\n",
            "-------------Epoch 15-------------\n",
            "current learning rate 1e-08\n",
            "batch: [    9], loss: 0.693\n",
            "temporary loss: 0.689915 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.696\n",
            "temporary loss: 0.693722 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.699\n",
            "temporary loss: 0.701980 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.695\n",
            "temporary loss: 0.694671 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.694\n",
            "temporary loss: 0.708696 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.697\n",
            "temporary loss: 0.697488 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.696\n",
            "temporary loss: 0.695589 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.695\n",
            "temporary loss: 0.698168 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.699\n",
            "temporary loss: 0.693089 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.695\n",
            "temporary loss: 0.700149 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 47.9%\n",
            "Avg loss: 0.695513 \n",
            "\n",
            "-------------Epoch 16-------------\n",
            "current learning rate 1e-08\n",
            "batch: [    9], loss: 0.697\n",
            "temporary loss: 0.698601 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.695\n",
            "temporary loss: 0.699285 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.696\n",
            "temporary loss: 0.693627 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.698\n",
            "temporary loss: 0.698884 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.696\n",
            "temporary loss: 0.696244 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.695\n",
            "temporary loss: 0.701059 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.694\n",
            "temporary loss: 0.697912 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.696\n",
            "temporary loss: 0.688870 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.696\n",
            "temporary loss: 0.690915 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.698\n",
            "temporary loss: 0.695361 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 47.8%\n",
            "Avg loss: 0.695511 \n",
            "\n",
            "-------------Epoch 17-------------\n",
            "current learning rate 1e-09\n",
            "batch: [    9], loss: 0.696\n",
            "temporary loss: 0.701782 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.697\n",
            "temporary loss: 0.702939 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.693\n",
            "temporary loss: 0.691158 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.696\n",
            "temporary loss: 0.687750 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.694\n",
            "temporary loss: 0.690212 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.696\n",
            "temporary loss: 0.699917 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.695\n",
            "temporary loss: 0.684301 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.698\n",
            "temporary loss: 0.703521 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.696\n",
            "temporary loss: 0.693599 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.698\n",
            "temporary loss: 0.697757 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 47.8%\n",
            "Avg loss: 0.695511 \n",
            "\n",
            "-------------Epoch 18-------------\n",
            "current learning rate 1e-09\n",
            "batch: [    9], loss: 0.694\n",
            "temporary loss: 0.690202 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.697\n",
            "temporary loss: 0.688232 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.698\n",
            "temporary loss: 0.699704 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.693\n",
            "temporary loss: 0.698550 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.694\n",
            "temporary loss: 0.693096 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.696\n",
            "temporary loss: 0.698867 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.697\n",
            "temporary loss: 0.696257 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.696\n",
            "temporary loss: 0.686471 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.697\n",
            "temporary loss: 0.696216 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.698\n",
            "temporary loss: 0.692932 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 47.8%\n",
            "Avg loss: 0.695511 \n",
            "\n",
            "-------------Epoch 19-------------\n",
            "current learning rate 1e-09\n",
            "batch: [    9], loss: 0.694\n",
            "temporary loss: 0.690359 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.696\n",
            "temporary loss: 0.697117 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.693\n",
            "temporary loss: 0.698437 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.696\n",
            "temporary loss: 0.696590 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.695\n",
            "temporary loss: 0.696085 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.698\n",
            "temporary loss: 0.697586 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.696\n",
            "temporary loss: 0.692477 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.700\n",
            "temporary loss: 0.697049 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.698\n",
            "temporary loss: 0.698648 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.694\n",
            "temporary loss: 0.695258 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 47.8%\n",
            "Avg loss: 0.695511 \n",
            "\n",
            "-------------Epoch 20-------------\n",
            "current learning rate 1e-10\n",
            "batch: [    9], loss: 0.695\n",
            "temporary loss: 0.695164 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.698\n",
            "temporary loss: 0.696367 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.696\n",
            "temporary loss: 0.697271 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.693\n",
            "temporary loss: 0.690380 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.698\n",
            "temporary loss: 0.684757 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.693\n",
            "temporary loss: 0.688593 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.700\n",
            "temporary loss: 0.702498 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.694\n",
            "temporary loss: 0.693606 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.698\n",
            "temporary loss: 0.698059 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.694\n",
            "temporary loss: 0.698598 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 47.8%\n",
            "Avg loss: 0.695511 \n",
            "\n",
            "Done!\n",
            "Time Consumption 747.459921836853\n"
          ]
        }
      ],
      "source": [
        "lr = 1e-6\n",
        "optimizer = optim.Adam(model_ViT.parameters(), lr=lr)\n",
        "lowest_loss = 1e4\n",
        "count = 0\n",
        "time_start = time.time()\n",
        "for t in range( 20 ):\n",
        "    print(f\"-------------Epoch {t+1}-------------\")\n",
        "    print(\"current learning rate\", lr)\n",
        "    #optimizer = optim.SGD(model.parameters(), lr=lr_list[t], momentum=0.9)\n",
        "    train_loop(train_loader, model_ViT, criterion, optimizer)\n",
        "    test_loss = test_loop_Entropy(test_loader, model_ViT, criterion)\n",
        "    if int(test_loss * 100) < lowest_loss:\n",
        "        lowest_loss = int(test_loss * 100)\n",
        "        count = 0\n",
        "    else:\n",
        "        count += 1\n",
        "    if count == 3:\n",
        "        lr /= 10\n",
        "        optimizer = optim.Adam(model_ViT.parameters(), lr=lr)\n",
        "        count = 0\n",
        "print(\"Done!\")\n",
        "time_end = time.time()\n",
        "print(\"Time Consumption\",time_end-time_start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVMFrpb04nAR"
      },
      "source": [
        "Post-norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "TMgihLio4oPi"
      },
      "outputs": [],
      "source": [
        "model_ViT = ViT_post(image_size=150, patch_size=6, num_classes=2, dim=32, depth=6, heads=16, mlp_dim=128, pool=\"cls\", channels=1, dim_head=16).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pAUQNA54oPj",
        "outputId": "7e4f13f0-ba9e-41ba-a364-0bcaeb43f404"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------Epoch 1-------------\n",
            "current learning rate 1e-05\n",
            "batch: [    9], loss: 0.697\n",
            "temporary loss: 0.694347 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.694\n",
            "temporary loss: 0.691093 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.693\n",
            "temporary loss: 0.689725 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.692\n",
            "temporary loss: 0.704651 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.696\n",
            "temporary loss: 0.687534 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.692\n",
            "temporary loss: 0.701314 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.691\n",
            "temporary loss: 0.687896 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.691\n",
            "temporary loss: 0.693600 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.693\n",
            "temporary loss: 0.689585 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.694\n",
            "temporary loss: 0.698655 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 52.6%\n",
            "Avg loss: 0.692148 \n",
            "\n",
            "-------------Epoch 2-------------\n",
            "current learning rate 1e-05\n",
            "batch: [    9], loss: 0.692\n",
            "temporary loss: 0.690657 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.691\n",
            "temporary loss: 0.687481 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.690\n",
            "temporary loss: 0.684122 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.691\n",
            "temporary loss: 0.694730 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.691\n",
            "temporary loss: 0.693154 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.688\n",
            "temporary loss: 0.690487 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.689\n",
            "temporary loss: 0.687991 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.693\n",
            "temporary loss: 0.691255 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.691\n",
            "temporary loss: 0.690987 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.689\n",
            "temporary loss: 0.694218 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 51.3%\n",
            "Avg loss: 0.690893 \n",
            "\n",
            "-------------Epoch 3-------------\n",
            "current learning rate 1e-05\n",
            "batch: [    9], loss: 0.692\n",
            "temporary loss: 0.693375 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.690\n",
            "temporary loss: 0.688101 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.688\n",
            "temporary loss: 0.685445 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.689\n",
            "temporary loss: 0.687430 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.687\n",
            "temporary loss: 0.694594 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.690\n",
            "temporary loss: 0.691481 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.687\n",
            "temporary loss: 0.682068 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.690\n",
            "temporary loss: 0.680489 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.685\n",
            "temporary loss: 0.684333 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.686\n",
            "temporary loss: 0.686305 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 56.5%\n",
            "Avg loss: 0.687338 \n",
            "\n",
            "-------------Epoch 4-------------\n",
            "current learning rate 1e-05\n",
            "batch: [    9], loss: 0.687\n",
            "temporary loss: 0.691917 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.688\n",
            "temporary loss: 0.696108 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.685\n",
            "temporary loss: 0.683952 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.685\n",
            "temporary loss: 0.685296 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.690\n",
            "temporary loss: 0.700186 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.687\n",
            "temporary loss: 0.681988 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.684\n",
            "temporary loss: 0.677629 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.685\n",
            "temporary loss: 0.678037 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.681\n",
            "temporary loss: 0.673133 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.681\n",
            "temporary loss: 0.688947 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 57.0%\n",
            "Avg loss: 0.684079 \n",
            "\n",
            "-------------Epoch 5-------------\n",
            "current learning rate 1e-05\n",
            "batch: [    9], loss: 0.682\n",
            "temporary loss: 0.686451 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.682\n",
            "temporary loss: 0.673036 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.682\n",
            "temporary loss: 0.684055 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.686\n",
            "temporary loss: 0.679318 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.680\n",
            "temporary loss: 0.679877 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.677\n",
            "temporary loss: 0.683034 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.681\n",
            "temporary loss: 0.681562 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.686\n",
            "temporary loss: 0.671880 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.677\n",
            "temporary loss: 0.682681 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.681\n",
            "temporary loss: 0.676794 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 58.6%\n",
            "Avg loss: 0.679121 \n",
            "\n",
            "-------------Epoch 6-------------\n",
            "current learning rate 1e-05\n",
            "batch: [    9], loss: 0.683\n",
            "temporary loss: 0.674104 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.672\n",
            "temporary loss: 0.666852 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.673\n",
            "temporary loss: 0.655764 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.679\n",
            "temporary loss: 0.674742 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.672\n",
            "temporary loss: 0.693251 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.685\n",
            "temporary loss: 0.676033 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.680\n",
            "temporary loss: 0.711815 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.681\n",
            "temporary loss: 0.688237 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.678\n",
            "temporary loss: 0.673526 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.673\n",
            "temporary loss: 0.670036 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 57.3%\n",
            "Avg loss: 0.677234 \n",
            "\n",
            "Done!\n",
            "Time Consumption 224.07204961776733\n"
          ]
        }
      ],
      "source": [
        "lr = 1e-5\n",
        "optimizer = optim.Adam(model_ViT.parameters(), lr=lr)\n",
        "lowest_loss = 1e4\n",
        "count = 0\n",
        "time_start = time.time()\n",
        "for t in range( 6 ):\n",
        "    print(f\"-------------Epoch {t+1}-------------\")\n",
        "    print(\"current learning rate\", lr)\n",
        "    #optimizer = optim.SGD(model.parameters(), lr=lr_list[t], momentum=0.9)\n",
        "    train_loop(train_loader, model_ViT, criterion, optimizer)\n",
        "    test_loss = test_loop_Entropy(test_loader, model_ViT, criterion)\n",
        "    if int(test_loss * 100) < lowest_loss:\n",
        "        lowest_loss = int(test_loss * 100)\n",
        "        count = 0\n",
        "    else:\n",
        "        count += 1\n",
        "    if count == 3:\n",
        "        lr /= 10\n",
        "        optimizer = optim.Adam(model_ViT.parameters(), lr=lr)\n",
        "        count = 0\n",
        "print(\"Done!\")\n",
        "time_end = time.time()\n",
        "print(\"Time Consumption\",time_end-time_start)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-5\n",
        "lowest_loss = 1e4\n",
        "count = 0\n",
        "time_start = time.time()\n",
        "for t in range( 20 ):\n",
        "    print(f\"-------------Epoch {t+1}-------------\")\n",
        "    print(\"current learning rate\", lr)\n",
        "    #optimizer = optim.SGD(model.parameters(), lr=lr_list[t], momentum=0.9)\n",
        "    train_loop(train_loader, model_ViT, criterion, optimizer)\n",
        "    test_loss = test_loop_Entropy(test_loader, model_ViT, criterion)\n",
        "    if int(test_loss * 100) < lowest_loss:\n",
        "        lowest_loss = int(test_loss * 100)\n",
        "        count = 0\n",
        "    else:\n",
        "        count += 1\n",
        "    if count == 3:\n",
        "        lr /= 10\n",
        "        optimizer = optim.Adam(model_ViT.parameters(), lr=lr)\n",
        "        count = 0\n",
        "print(\"Done!\")\n",
        "time_end = time.time()\n",
        "print(\"Time Consumption\",time_end-time_start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igv1J20_7vpS",
        "outputId": "fded0c89-662f-43db-a482-0c21bd6e0559"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------Epoch 1-------------\n",
            "current learning rate 1e-05\n",
            "batch: [    9], loss: 0.669\n",
            "temporary loss: 0.656839 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.672\n",
            "temporary loss: 0.696362 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.677\n",
            "temporary loss: 0.675114 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.669\n",
            "temporary loss: 0.662364 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.665\n",
            "temporary loss: 0.676390 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.670\n",
            "temporary loss: 0.648163 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.663\n",
            "temporary loss: 0.682883 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.668\n",
            "temporary loss: 0.663478 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.661\n",
            "temporary loss: 0.672832 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.658\n",
            "temporary loss: 0.666985 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 62.1%\n",
            "Avg loss: 0.665385 \n",
            "\n",
            "-------------Epoch 2-------------\n",
            "current learning rate 1e-05\n",
            "batch: [    9], loss: 0.654\n",
            "temporary loss: 0.637632 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.671\n",
            "temporary loss: 0.659977 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.669\n",
            "temporary loss: 0.701922 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.667\n",
            "temporary loss: 0.652638 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.658\n",
            "temporary loss: 0.685325 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.663\n",
            "temporary loss: 0.624362 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.652\n",
            "temporary loss: 0.659144 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.654\n",
            "temporary loss: 0.684522 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.636\n",
            "temporary loss: 0.578837 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.642\n",
            "temporary loss: 0.653274 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 59.6%\n",
            "Avg loss: 0.657134 \n",
            "\n",
            "-------------Epoch 3-------------\n",
            "current learning rate 1e-05\n",
            "batch: [    9], loss: 0.650\n",
            "temporary loss: 0.663310 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.651\n",
            "temporary loss: 0.621566 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.643\n",
            "temporary loss: 0.678673 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.617\n",
            "temporary loss: 0.599723 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.630\n",
            "temporary loss: 0.667498 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.645\n",
            "temporary loss: 0.615968 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.617\n",
            "temporary loss: 0.651474 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.649\n",
            "temporary loss: 0.727673 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.633\n",
            "temporary loss: 0.631168 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.638\n",
            "temporary loss: 0.616579 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 64.8%\n",
            "Avg loss: 0.629340 \n",
            "\n",
            "-------------Epoch 4-------------\n",
            "current learning rate 1e-05\n",
            "batch: [    9], loss: 0.642\n",
            "temporary loss: 0.661474 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.623\n",
            "temporary loss: 0.621279 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.630\n",
            "temporary loss: 0.663034 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.605\n",
            "temporary loss: 0.652579 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.612\n",
            "temporary loss: 0.642184 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.615\n",
            "temporary loss: 0.615763 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.630\n",
            "temporary loss: 0.563521 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.602\n",
            "temporary loss: 0.586628 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.594\n",
            "temporary loss: 0.645111 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.594\n",
            "temporary loss: 0.573718 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 66.9%\n",
            "Avg loss: 0.610762 \n",
            "\n",
            "-------------Epoch 5-------------\n",
            "current learning rate 1e-05\n",
            "batch: [    9], loss: 0.599\n",
            "temporary loss: 0.628589 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.581\n",
            "temporary loss: 0.578263 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.590\n",
            "temporary loss: 0.599244 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.574\n",
            "temporary loss: 0.584125 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.593\n",
            "temporary loss: 0.625115 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.589\n",
            "temporary loss: 0.535593 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.598\n",
            "temporary loss: 0.572578 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.592\n",
            "temporary loss: 0.607818 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.588\n",
            "temporary loss: 0.594466 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.574\n",
            "temporary loss: 0.598170 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 66.6%\n",
            "Avg loss: 0.589439 \n",
            "\n",
            "-------------Epoch 6-------------\n",
            "current learning rate 1e-05\n",
            "batch: [    9], loss: 0.574\n",
            "temporary loss: 0.577694 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.562\n",
            "temporary loss: 0.532872 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.557\n",
            "temporary loss: 0.545600 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.586\n",
            "temporary loss: 0.534932 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.592\n",
            "temporary loss: 0.596532 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.544\n",
            "temporary loss: 0.501403 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.564\n",
            "temporary loss: 0.584198 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.560\n",
            "temporary loss: 0.594585 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.560\n",
            "temporary loss: 0.541150 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.582\n",
            "temporary loss: 0.522519 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 67.8%\n",
            "Avg loss: 0.580790 \n",
            "\n",
            "-------------Epoch 7-------------\n",
            "current learning rate 1e-05\n",
            "batch: [    9], loss: 0.549\n",
            "temporary loss: 0.595270 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.563\n",
            "temporary loss: 0.441254 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.541\n",
            "temporary loss: 0.496814 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.559\n",
            "temporary loss: 0.526149 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.530\n",
            "temporary loss: 0.526832 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.567\n",
            "temporary loss: 0.603170 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.527\n",
            "temporary loss: 0.480695 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.534\n",
            "temporary loss: 0.544874 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.557\n",
            "temporary loss: 0.560531 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.540\n",
            "temporary loss: 0.529809 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 69.9%\n",
            "Avg loss: 0.559161 \n",
            "\n",
            "-------------Epoch 8-------------\n",
            "current learning rate 1e-05\n",
            "batch: [    9], loss: 0.538\n",
            "temporary loss: 0.528185 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.545\n",
            "temporary loss: 0.586454 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.561\n",
            "temporary loss: 0.576758 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.544\n",
            "temporary loss: 0.594694 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.492\n",
            "temporary loss: 0.458016 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.535\n",
            "temporary loss: 0.511466 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.524\n",
            "temporary loss: 0.477858 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.506\n",
            "temporary loss: 0.474999 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.543\n",
            "temporary loss: 0.580862 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.513\n",
            "temporary loss: 0.487613 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 69.3%\n",
            "Avg loss: 0.571184 \n",
            "\n",
            "-------------Epoch 9-------------\n",
            "current learning rate 1e-05\n",
            "batch: [    9], loss: 0.513\n",
            "temporary loss: 0.481619 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.524\n",
            "temporary loss: 0.491734 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.483\n",
            "temporary loss: 0.546491 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.539\n",
            "temporary loss: 0.484233 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.491\n",
            "temporary loss: 0.505349 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.523\n",
            "temporary loss: 0.453966 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.525\n",
            "temporary loss: 0.484722 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.514\n",
            "temporary loss: 0.499277 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.513\n",
            "temporary loss: 0.560845 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.545\n",
            "temporary loss: 0.575928 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 71.2%\n",
            "Avg loss: 0.539416 \n",
            "\n",
            "-------------Epoch 10-------------\n",
            "current learning rate 1e-05\n",
            "batch: [    9], loss: 0.497\n",
            "temporary loss: 0.534819 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.501\n",
            "temporary loss: 0.489468 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.507\n",
            "temporary loss: 0.414371 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.498\n",
            "temporary loss: 0.525263 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.505\n",
            "temporary loss: 0.552691 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.501\n",
            "temporary loss: 0.585333 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.536\n",
            "temporary loss: 0.511164 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.519\n",
            "temporary loss: 0.536369 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.501\n",
            "temporary loss: 0.492997 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.507\n",
            "temporary loss: 0.494536 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 70.8%\n",
            "Avg loss: 0.533081 \n",
            "\n",
            "-------------Epoch 11-------------\n",
            "current learning rate 1e-05\n",
            "batch: [    9], loss: 0.490\n",
            "temporary loss: 0.463589 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.502\n",
            "temporary loss: 0.437997 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.481\n",
            "temporary loss: 0.509488 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.497\n",
            "temporary loss: 0.457398 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.506\n",
            "temporary loss: 0.560156 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.477\n",
            "temporary loss: 0.438794 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.469\n",
            "temporary loss: 0.473614 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.506\n",
            "temporary loss: 0.498843 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.505\n",
            "temporary loss: 0.489062 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.486\n",
            "temporary loss: 0.435260 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 71.6%\n",
            "Avg loss: 0.533378 \n",
            "\n",
            "-------------Epoch 12-------------\n",
            "current learning rate 1e-05\n",
            "batch: [    9], loss: 0.483\n",
            "temporary loss: 0.456252 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.488\n",
            "temporary loss: 0.476665 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.493\n",
            "temporary loss: 0.457275 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.485\n",
            "temporary loss: 0.528536 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.458\n",
            "temporary loss: 0.411831 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.493\n",
            "temporary loss: 0.471812 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.487\n",
            "temporary loss: 0.424456 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.486\n",
            "temporary loss: 0.549826 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.476\n",
            "temporary loss: 0.472447 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.486\n",
            "temporary loss: 0.505972 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 70.6%\n",
            "Avg loss: 0.550071 \n",
            "\n",
            "-------------Epoch 13-------------\n",
            "current learning rate 1.0000000000000002e-06\n",
            "batch: [    9], loss: 0.489\n",
            "temporary loss: 0.466970 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.458\n",
            "temporary loss: 0.424271 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.444\n",
            "temporary loss: 0.460357 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.484\n",
            "temporary loss: 0.455226 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.484\n",
            "temporary loss: 0.526977 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.453\n",
            "temporary loss: 0.467617 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.452\n",
            "temporary loss: 0.398364 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.493\n",
            "temporary loss: 0.501526 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.472\n",
            "temporary loss: 0.481250 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.472\n",
            "temporary loss: 0.474128 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 71.7%\n",
            "Avg loss: 0.521836 \n",
            "\n",
            "-------------Epoch 14-------------\n",
            "current learning rate 1.0000000000000002e-06\n",
            "batch: [    9], loss: 0.474\n",
            "temporary loss: 0.406766 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.467\n",
            "temporary loss: 0.439994 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.462\n",
            "temporary loss: 0.501650 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.483\n",
            "temporary loss: 0.477438 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.470\n",
            "temporary loss: 0.442897 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.468\n",
            "temporary loss: 0.509110 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.464\n",
            "temporary loss: 0.439965 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.464\n",
            "temporary loss: 0.459435 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.448\n",
            "temporary loss: 0.388547 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.465\n",
            "temporary loss: 0.524296 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 72.4%\n",
            "Avg loss: 0.519981 \n",
            "\n",
            "-------------Epoch 15-------------\n",
            "current learning rate 1.0000000000000002e-06\n",
            "batch: [    9], loss: 0.463\n",
            "temporary loss: 0.469007 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.495\n",
            "temporary loss: 0.438995 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.459\n",
            "temporary loss: 0.397532 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.456\n",
            "temporary loss: 0.406045 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.475\n",
            "temporary loss: 0.413637 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.475\n",
            "temporary loss: 0.477896 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.457\n",
            "temporary loss: 0.443704 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.468\n",
            "temporary loss: 0.421710 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.454\n",
            "temporary loss: 0.508500 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.455\n",
            "temporary loss: 0.422965 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 72.2%\n",
            "Avg loss: 0.519235 \n",
            "\n",
            "-------------Epoch 16-------------\n",
            "current learning rate 1.0000000000000002e-06\n",
            "batch: [    9], loss: 0.464\n",
            "temporary loss: 0.445552 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.477\n",
            "temporary loss: 0.505871 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.461\n",
            "temporary loss: 0.360414 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.463\n",
            "temporary loss: 0.427682 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.455\n",
            "temporary loss: 0.438742 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.471\n",
            "temporary loss: 0.479809 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.458\n",
            "temporary loss: 0.477033 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.468\n",
            "temporary loss: 0.483932 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.478\n",
            "temporary loss: 0.566238 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.448\n",
            "temporary loss: 0.423576 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 71.7%\n",
            "Avg loss: 0.517948 \n",
            "\n",
            "-------------Epoch 17-------------\n",
            "current learning rate 1.0000000000000002e-06\n",
            "batch: [    9], loss: 0.460\n",
            "temporary loss: 0.453058 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.489\n",
            "temporary loss: 0.468343 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.462\n",
            "temporary loss: 0.455130 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.469\n",
            "temporary loss: 0.515238 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.446\n",
            "temporary loss: 0.429682 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.462\n",
            "temporary loss: 0.490151 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.441\n",
            "temporary loss: 0.539954 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.466\n",
            "temporary loss: 0.437780 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.468\n",
            "temporary loss: 0.481483 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.476\n",
            "temporary loss: 0.499957 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 71.8%\n",
            "Avg loss: 0.521616 \n",
            "\n",
            "-------------Epoch 18-------------\n",
            "current learning rate 1.0000000000000002e-07\n",
            "batch: [    9], loss: 0.466\n",
            "temporary loss: 0.483249 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.478\n",
            "temporary loss: 0.427542 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.492\n",
            "temporary loss: 0.410164 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.442\n",
            "temporary loss: 0.400676 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.458\n",
            "temporary loss: 0.507255 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.480\n",
            "temporary loss: 0.498524 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.421\n",
            "temporary loss: 0.429068 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.475\n",
            "temporary loss: 0.504095 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.458\n",
            "temporary loss: 0.444849 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.453\n",
            "temporary loss: 0.388724 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 72.4%\n",
            "Avg loss: 0.518591 \n",
            "\n",
            "-------------Epoch 19-------------\n",
            "current learning rate 1.0000000000000002e-07\n",
            "batch: [    9], loss: 0.455\n",
            "temporary loss: 0.437318 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.472\n",
            "temporary loss: 0.413233 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.468\n",
            "temporary loss: 0.467334 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.446\n",
            "temporary loss: 0.461626 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.445\n",
            "temporary loss: 0.412058 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.470\n",
            "temporary loss: 0.549891 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.464\n",
            "temporary loss: 0.462205 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.456\n",
            "temporary loss: 0.513053 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.465\n",
            "temporary loss: 0.444457 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.476\n",
            "temporary loss: 0.428927 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 72.3%\n",
            "Avg loss: 0.518528 \n",
            "\n",
            "-------------Epoch 20-------------\n",
            "current learning rate 1.0000000000000002e-07\n",
            "batch: [    9], loss: 0.452\n",
            "temporary loss: 0.394162 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.469\n",
            "temporary loss: 0.493945 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.452\n",
            "temporary loss: 0.517261 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.465\n",
            "temporary loss: 0.401456 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.459\n",
            "temporary loss: 0.455997 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.452\n",
            "temporary loss: 0.416130 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.478\n",
            "temporary loss: 0.500136 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.480\n",
            "temporary loss: 0.482380 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.454\n",
            "temporary loss: 0.465079 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.456\n",
            "temporary loss: 0.394318 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 72.4%\n",
            "Avg loss: 0.518284 \n",
            "\n",
            "Done!\n",
            "Time Consumption 747.4754474163055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erASHLeXQfff"
      },
      "source": [
        "## IV. MLP-mixer\n",
        "The MLP-mixer architecture comes from https://arxiv.org/abs/2105.01601 . We implement this architecture.\n",
        "\n",
        "Batch size: 200\n",
        "\n",
        "Training on: A100 GPU\n",
        "\n",
        "Learning Rate: if the test loss isn't improved for three consecutive epochs, the learning rate will be reduced by a factor of ten.\n",
        "\n",
        "Optimizer: Adam\n",
        "\n",
        "Test Accuracy: 91.4\n",
        "\n",
        "ROC-AUC score: 0.972\n",
        "\n",
        "MLP-mixer has a really good performance! And the computaional efficiency is really good."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "x90y8oc_Z2E8"
      },
      "outputs": [],
      "source": [
        "def pair(t):\n",
        "    return t if isinstance(t, tuple) else (t, t)\n",
        "\n",
        "class PostNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.norm( self.fn(x, **kwargs) )\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "QjCHXHGtQ4Pd"
      },
      "outputs": [],
      "source": [
        "class MixerBlock(nn.Module):\n",
        "    def __init__(self, patch_dim, MLP_dim1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.MLP1 = nn.Sequential(\n",
        "            nn.Linear(patch_dim, MLP_dim1),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(MLP_dim1, patch_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = rearrange(x, \"b n p -> b p n\")\n",
        "        x = self.MLP1(x)\n",
        "        x = rearrange(x, \"b p n -> b n p\")\n",
        "\n",
        "        return x\n",
        "\n",
        "class MLPBlock(nn.Module):\n",
        "    def __init__(self, dim, MLP_dim2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.MLP2 = nn.Sequential(\n",
        "            nn.Linear(dim, MLP_dim2),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(MLP_dim2, dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.MLP2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "iBmilkw2aKtN"
      },
      "outputs": [],
      "source": [
        "class Transformer_post(nn.Module):\n",
        "    def __init__(self, dim, patch_dim, depth, MLP_dim1, MLP_dim2):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PostNorm(dim, MixerBlock(patch_dim, MLP_dim1) ),\n",
        "                PostNorm(dim, MLPBlock(dim, MLP_dim2) )\n",
        "            ]))\n",
        "    def forward(self, x):\n",
        "        for mixer, ff in self.layers:\n",
        "            x = mixer(x) + x\n",
        "            x = ff(x) + x\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "5VAMGGxMaKtN"
      },
      "outputs": [],
      "source": [
        "class MLP_mixer_post(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, MLP_dim1, MLP_dim2, pool = 'cls', channels = 3):\n",
        "        super().__init__()\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
        "        patch_dim = channels * patch_height * patch_width\n",
        "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
        "            nn.LayerNorm(patch_dim),\n",
        "            nn.Linear(patch_dim, dim),\n",
        "            nn.LayerNorm(dim),\n",
        "        )\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "\n",
        "        self.transformer = Transformer_post(dim, num_patches + 1, depth, MLP_dim1, MLP_dim2)\n",
        "\n",
        "        self.pool = pool\n",
        "        self.to_latent = nn.Identity()\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.to_patch_embedding(img)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
        "\n",
        "        x = self.to_latent(x)\n",
        "        return self.mlp_head(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "NX3MblD3ZQN1"
      },
      "outputs": [],
      "source": [
        "model_mixer = MLP_mixer_post(image_size=150, patch_size=6, num_classes=2, dim=32, depth=6, MLP_dim1=128, MLP_dim2=1000, channels=1).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plP0H03SdVCs",
        "outputId": "7f834bca-2021-4127-b513-b1b3eaf97d53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------Epoch 1-------------\n",
            "current learning rate 0.0001\n",
            "batch: [    9], loss: 0.709\n",
            "temporary loss: 0.684861 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.686\n",
            "temporary loss: 0.677836 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.681\n",
            "temporary loss: 0.697925 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.669\n",
            "temporary loss: 0.662154 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.659\n",
            "temporary loss: 0.615148 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.626\n",
            "temporary loss: 0.599907 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.570\n",
            "temporary loss: 0.557729 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.528\n",
            "temporary loss: 0.512480 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.483\n",
            "temporary loss: 0.520097 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.485\n",
            "temporary loss: 0.399080 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 74.0%\n",
            "Avg loss: 0.466902 \n",
            "\n",
            "-------------Epoch 2-------------\n",
            "current learning rate 0.0001\n",
            "batch: [    9], loss: 0.392\n",
            "temporary loss: 0.334982 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.417\n",
            "temporary loss: 0.444035 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.407\n",
            "temporary loss: 0.378896 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.355\n",
            "temporary loss: 0.275222 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.337\n",
            "temporary loss: 0.290967 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.345\n",
            "temporary loss: 0.323802 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.309\n",
            "temporary loss: 0.300164 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.290\n",
            "temporary loss: 0.304896 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.375\n",
            "temporary loss: 0.353776 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.348\n",
            "temporary loss: 0.348392 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 82.1%\n",
            "Avg loss: 0.365060 \n",
            "\n",
            "-------------Epoch 3-------------\n",
            "current learning rate 0.0001\n",
            "batch: [    9], loss: 0.241\n",
            "temporary loss: 0.278629 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.227\n",
            "temporary loss: 0.204056 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.237\n",
            "temporary loss: 0.209242 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.230\n",
            "temporary loss: 0.291226 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.241\n",
            "temporary loss: 0.266144 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.214\n",
            "temporary loss: 0.200588 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.217\n",
            "temporary loss: 0.137807 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.226\n",
            "temporary loss: 0.183637 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.213\n",
            "temporary loss: 0.202396 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.234\n",
            "temporary loss: 0.300761 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 86.8%\n",
            "Avg loss: 0.306663 \n",
            "\n",
            "-------------Epoch 4-------------\n",
            "current learning rate 0.0001\n",
            "batch: [    9], loss: 0.164\n",
            "temporary loss: 0.133432 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.181\n",
            "temporary loss: 0.238483 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.148\n",
            "temporary loss: 0.131808 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.129\n",
            "temporary loss: 0.089648 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.154\n",
            "temporary loss: 0.147936 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.223\n",
            "temporary loss: 0.227845 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.191\n",
            "temporary loss: 0.188863 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.157\n",
            "temporary loss: 0.177164 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.117\n",
            "temporary loss: 0.119156 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.118\n",
            "temporary loss: 0.081619 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 86.2%\n",
            "Avg loss: 0.329448 \n",
            "\n",
            "-------------Epoch 5-------------\n",
            "current learning rate 0.0001\n",
            "batch: [    9], loss: 0.098\n",
            "temporary loss: 0.097024 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.119\n",
            "temporary loss: 0.241194 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.112\n",
            "temporary loss: 0.120212 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.120\n",
            "temporary loss: 0.144527 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.154\n",
            "temporary loss: 0.082267 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.106\n",
            "temporary loss: 0.124863 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.094\n",
            "temporary loss: 0.098714 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.098\n",
            "temporary loss: 0.157077 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.162\n",
            "temporary loss: 0.176760 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.105\n",
            "temporary loss: 0.065506 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 87.4%\n",
            "Avg loss: 0.305192 \n",
            "\n",
            "-------------Epoch 6-------------\n",
            "current learning rate 0.0001\n",
            "batch: [    9], loss: 0.068\n",
            "temporary loss: 0.075463 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.089\n",
            "temporary loss: 0.131144 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.100\n",
            "temporary loss: 0.071666 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.078\n",
            "temporary loss: 0.029809 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.079\n",
            "temporary loss: 0.062492 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.074\n",
            "temporary loss: 0.051133 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.091\n",
            "temporary loss: 0.152678 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.119\n",
            "temporary loss: 0.154097 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.119\n",
            "temporary loss: 0.172506 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.094\n",
            "temporary loss: 0.048588 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 90.3%\n",
            "Avg loss: 0.251233 \n",
            "\n",
            "-------------Epoch 7-------------\n",
            "current learning rate 0.0001\n",
            "batch: [    9], loss: 0.053\n",
            "temporary loss: 0.036049 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.059\n",
            "temporary loss: 0.083558 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.058\n",
            "temporary loss: 0.096885 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.080\n",
            "temporary loss: 0.020260 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.056\n",
            "temporary loss: 0.037073 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.044\n",
            "temporary loss: 0.052615 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.046\n",
            "temporary loss: 0.105378 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.108\n",
            "temporary loss: 0.156630 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.151\n",
            "temporary loss: 0.128917 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.101\n",
            "temporary loss: 0.143038 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 88.5%\n",
            "Avg loss: 0.296745 \n",
            "\n",
            "-------------Epoch 8-------------\n",
            "current learning rate 0.0001\n",
            "batch: [    9], loss: 0.058\n",
            "temporary loss: 0.062504 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.102\n",
            "temporary loss: 0.071351 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.058\n",
            "temporary loss: 0.054629 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.041\n",
            "temporary loss: 0.020360 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.061\n",
            "temporary loss: 0.123016 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.064\n",
            "temporary loss: 0.135349 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.077\n",
            "temporary loss: 0.053614 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.061\n",
            "temporary loss: 0.070543 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.057\n",
            "temporary loss: 0.088060 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.093\n",
            "temporary loss: 0.191446 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 89.9%\n",
            "Avg loss: 0.302417 \n",
            "\n",
            "-------------Epoch 9-------------\n",
            "current learning rate 0.0001\n",
            "batch: [    9], loss: 0.068\n",
            "temporary loss: 0.147451 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.141\n",
            "temporary loss: 0.177634 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.077\n",
            "temporary loss: 0.084130 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.055\n",
            "temporary loss: 0.056697 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.038\n",
            "temporary loss: 0.029666 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.042\n",
            "temporary loss: 0.036026 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.040\n",
            "temporary loss: 0.014394 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.086\n",
            "temporary loss: 0.128912 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.066\n",
            "temporary loss: 0.031100 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.040\n",
            "temporary loss: 0.033390 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 90.6%\n",
            "Avg loss: 0.285197 \n",
            "\n",
            "-------------Epoch 10-------------\n",
            "current learning rate 1e-05\n",
            "batch: [    9], loss: 0.022\n",
            "temporary loss: 0.010688 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.015\n",
            "temporary loss: 0.023093 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.014\n",
            "temporary loss: 0.010084 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.015\n",
            "temporary loss: 0.008806 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.013\n",
            "temporary loss: 0.009589 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.015\n",
            "temporary loss: 0.010228 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.010\n",
            "temporary loss: 0.008905 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.009\n",
            "temporary loss: 0.007997 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.011\n",
            "temporary loss: 0.009040 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.016\n",
            "temporary loss: 0.008023 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 90.7%\n",
            "Avg loss: 0.310171 \n",
            "\n",
            "-------------Epoch 11-------------\n",
            "current learning rate 1e-05\n",
            "batch: [    9], loss: 0.008\n",
            "temporary loss: 0.007602 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.008\n",
            "temporary loss: 0.011381 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.009\n",
            "temporary loss: 0.008442 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.009\n",
            "temporary loss: 0.007118 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.008\n",
            "temporary loss: 0.007209 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.007\n",
            "temporary loss: 0.005971 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.008\n",
            "temporary loss: 0.005587 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.007\n",
            "temporary loss: 0.006188 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.008\n",
            "temporary loss: 0.005860 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.007\n",
            "temporary loss: 0.006888 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 91.0%\n",
            "Avg loss: 0.344013 \n",
            "\n",
            "-------------Epoch 12-------------\n",
            "current learning rate 1e-05\n",
            "batch: [    9], loss: 0.007\n",
            "temporary loss: 0.006072 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.007\n",
            "temporary loss: 0.006377 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.006\n",
            "temporary loss: 0.006519 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.006\n",
            "temporary loss: 0.005707 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.006\n",
            "temporary loss: 0.005904 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.006\n",
            "temporary loss: 0.005523 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.006\n",
            "temporary loss: 0.005441 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.006\n",
            "temporary loss: 0.005344 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.006\n",
            "temporary loss: 0.005010 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.006\n",
            "temporary loss: 0.005806 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 91.2%\n",
            "Avg loss: 0.336134 \n",
            "\n",
            "-------------Epoch 13-------------\n",
            "current learning rate 1.0000000000000002e-06\n",
            "batch: [    9], loss: 0.005\n",
            "temporary loss: 0.005570 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.006\n",
            "temporary loss: 0.005451 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.005\n",
            "temporary loss: 0.005376 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.005\n",
            "temporary loss: 0.005128 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.005\n",
            "temporary loss: 0.004865 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.005\n",
            "temporary loss: 0.005664 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.005\n",
            "temporary loss: 0.004999 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.005\n",
            "temporary loss: 0.004952 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.005\n",
            "temporary loss: 0.005981 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.005\n",
            "temporary loss: 0.005018 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 91.3%\n",
            "Avg loss: 0.340593 \n",
            "\n",
            "-------------Epoch 14-------------\n",
            "current learning rate 1.0000000000000002e-06\n",
            "batch: [    9], loss: 0.005\n",
            "temporary loss: 0.004884 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.005\n",
            "temporary loss: 0.005042 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.005\n",
            "temporary loss: 0.005459 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.005\n",
            "temporary loss: 0.005652 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.005\n",
            "temporary loss: 0.005279 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.005\n",
            "temporary loss: 0.005006 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.005\n",
            "temporary loss: 0.005127 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.005\n",
            "temporary loss: 0.005244 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.005\n",
            "temporary loss: 0.004599 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.005\n",
            "temporary loss: 0.005308 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 91.4%\n",
            "Avg loss: 0.346934 \n",
            "\n",
            "-------------Epoch 15-------------\n",
            "current learning rate 1.0000000000000002e-06\n",
            "batch: [    9], loss: 0.005\n",
            "temporary loss: 0.004718 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.005\n",
            "temporary loss: 0.004842 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.005\n",
            "temporary loss: 0.004537 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.005\n",
            "temporary loss: 0.004651 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.005\n",
            "temporary loss: 0.004941 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.005\n",
            "temporary loss: 0.004578 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.005\n",
            "temporary loss: 0.004730 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.005\n",
            "temporary loss: 0.004806 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.005\n",
            "temporary loss: 0.004381 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.005\n",
            "temporary loss: 0.004440 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 91.5%\n",
            "Avg loss: 0.352807 \n",
            "\n",
            "-------------Epoch 16-------------\n",
            "current learning rate 1.0000000000000002e-07\n",
            "batch: [    9], loss: 0.005\n",
            "temporary loss: 0.004610 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.005\n",
            "temporary loss: 0.004549 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.005\n",
            "temporary loss: 0.004539 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.005\n",
            "temporary loss: 0.004849 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.005\n",
            "temporary loss: 0.004343 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.005\n",
            "temporary loss: 0.004595 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.004\n",
            "temporary loss: 0.004357 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.005\n",
            "temporary loss: 0.004390 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.005\n",
            "temporary loss: 0.004411 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.005\n",
            "temporary loss: 0.004672 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 91.4%\n",
            "Avg loss: 0.354414 \n",
            "\n",
            "-------------Epoch 17-------------\n",
            "current learning rate 1.0000000000000002e-07\n",
            "batch: [    9], loss: 0.005\n",
            "temporary loss: 0.004516 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.005\n",
            "temporary loss: 0.004378 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.005\n",
            "temporary loss: 0.004292 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.005\n",
            "temporary loss: 0.004498 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.005\n",
            "temporary loss: 0.004437 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.005\n",
            "temporary loss: 0.004510 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.005\n",
            "temporary loss: 0.004229 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.005\n",
            "temporary loss: 0.004529 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.005\n",
            "temporary loss: 0.004543 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.005\n",
            "temporary loss: 0.004812 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 91.4%\n",
            "Avg loss: 0.355269 \n",
            "\n",
            "-------------Epoch 18-------------\n",
            "current learning rate 1.0000000000000002e-07\n",
            "batch: [    9], loss: 0.005\n",
            "temporary loss: 0.004682 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.005\n",
            "temporary loss: 0.004810 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.005\n",
            "temporary loss: 0.004422 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.005\n",
            "temporary loss: 0.004516 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.005\n",
            "temporary loss: 0.004504 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.005\n",
            "temporary loss: 0.004733 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.005\n",
            "temporary loss: 0.004591 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.005\n",
            "temporary loss: 0.004890 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.005\n",
            "temporary loss: 0.004508 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.005\n",
            "temporary loss: 0.004300 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 91.4%\n",
            "Avg loss: 0.356505 \n",
            "\n",
            "-------------Epoch 19-------------\n",
            "current learning rate 1.0000000000000002e-08\n",
            "batch: [    9], loss: 0.004\n",
            "temporary loss: 0.004292 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.005\n",
            "temporary loss: 0.004487 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.005\n",
            "temporary loss: 0.004810 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.005\n",
            "temporary loss: 0.004409 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.005\n",
            "temporary loss: 0.004590 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.005\n",
            "temporary loss: 0.004334 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.004\n",
            "temporary loss: 0.004330 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.005\n",
            "temporary loss: 0.004513 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.004\n",
            "temporary loss: 0.004385 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.005\n",
            "temporary loss: 0.004276 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 91.4%\n",
            "Avg loss: 0.356533 \n",
            "\n",
            "-------------Epoch 20-------------\n",
            "current learning rate 1.0000000000000002e-08\n",
            "batch: [    9], loss: 0.005\n",
            "temporary loss: 0.004596 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.004\n",
            "temporary loss: 0.004625 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.005\n",
            "temporary loss: 0.004589 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.005\n",
            "temporary loss: 0.004425 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.005\n",
            "temporary loss: 0.004353 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.005\n",
            "temporary loss: 0.004476 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.005\n",
            "temporary loss: 0.004974 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.005\n",
            "temporary loss: 0.004690 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.005\n",
            "temporary loss: 0.004463 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.004\n",
            "temporary loss: 0.004608 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 91.4%\n",
            "Avg loss: 0.356624 \n",
            "\n",
            "-------------Epoch 21-------------\n",
            "current learning rate 1.0000000000000002e-08\n",
            "batch: [    9], loss: 0.005\n",
            "temporary loss: 0.004698 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.005\n",
            "temporary loss: 0.004615 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.005\n",
            "temporary loss: 0.004510 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.005\n",
            "temporary loss: 0.004295 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.005\n",
            "temporary loss: 0.004320 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.004\n",
            "temporary loss: 0.004435 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.005\n",
            "temporary loss: 0.004495 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.005\n",
            "temporary loss: 0.004258 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.005\n",
            "temporary loss: 0.004464 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.005\n",
            "temporary loss: 0.004783 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 91.4%\n",
            "Avg loss: 0.356677 \n",
            "\n",
            "-------------Epoch 22-------------\n",
            "current learning rate 1.0000000000000003e-09\n",
            "batch: [    9], loss: 0.005\n",
            "temporary loss: 0.004436 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.004\n",
            "temporary loss: 0.004785 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.005\n",
            "temporary loss: 0.004530 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.005\n",
            "temporary loss: 0.004745 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.005\n",
            "temporary loss: 0.004764 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.005\n",
            "temporary loss: 0.004638 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.004\n",
            "temporary loss: 0.004506 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.005\n",
            "temporary loss: 0.004437 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.005\n",
            "temporary loss: 0.004722 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.004\n",
            "temporary loss: 0.004443 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 91.4%\n",
            "Avg loss: 0.356683 \n",
            "\n",
            "-------------Epoch 23-------------\n",
            "current learning rate 1.0000000000000003e-09\n",
            "batch: [    9], loss: 0.005\n",
            "temporary loss: 0.004573 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.005\n",
            "temporary loss: 0.004867 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.005\n",
            "temporary loss: 0.004646 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.005\n",
            "temporary loss: 0.004733 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.004\n",
            "temporary loss: 0.004419 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.005\n",
            "temporary loss: 0.004576 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.005\n",
            "temporary loss: 0.004590 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.005\n",
            "temporary loss: 0.004731 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.004\n",
            "temporary loss: 0.004229 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.005\n",
            "temporary loss: 0.004784 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 91.4%\n",
            "Avg loss: 0.356686 \n",
            "\n",
            "-------------Epoch 24-------------\n",
            "current learning rate 1.0000000000000003e-09\n",
            "batch: [    9], loss: 0.004\n",
            "temporary loss: 0.004327 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.005\n",
            "temporary loss: 0.004751 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.005\n",
            "temporary loss: 0.004684 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.005\n",
            "temporary loss: 0.004638 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.005\n",
            "temporary loss: 0.004573 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.005\n",
            "temporary loss: 0.004393 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.005\n",
            "temporary loss: 0.004458 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.005\n",
            "temporary loss: 0.004461 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.005\n",
            "temporary loss: 0.004494 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.004\n",
            "temporary loss: 0.004278 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 91.4%\n",
            "Avg loss: 0.356690 \n",
            "\n",
            "-------------Epoch 25-------------\n",
            "current learning rate 1.0000000000000003e-10\n",
            "batch: [    9], loss: 0.005\n",
            "temporary loss: 0.004906 | [  900/ 9000]\n",
            "batch: [   18], loss: 0.005\n",
            "temporary loss: 0.004902 | [ 1800/ 9000]\n",
            "batch: [   27], loss: 0.005\n",
            "temporary loss: 0.004720 | [ 2700/ 9000]\n",
            "batch: [   36], loss: 0.005\n",
            "temporary loss: 0.004570 | [ 3600/ 9000]\n",
            "batch: [   45], loss: 0.005\n",
            "temporary loss: 0.004328 | [ 4500/ 9000]\n",
            "batch: [   54], loss: 0.004\n",
            "temporary loss: 0.004326 | [ 5400/ 9000]\n",
            "batch: [   63], loss: 0.005\n",
            "temporary loss: 0.004429 | [ 6300/ 9000]\n",
            "batch: [   72], loss: 0.004\n",
            "temporary loss: 0.004628 | [ 7200/ 9000]\n",
            "batch: [   81], loss: 0.004\n",
            "temporary loss: 0.004529 | [ 8100/ 9000]\n",
            "batch: [   90], loss: 0.004\n",
            "temporary loss: 0.004785 | [ 9000/ 9000]\n",
            "Test Error: \n",
            " Accuracy: 91.4%\n",
            "Avg loss: 0.356690 \n",
            "\n",
            "Done!\n",
            "Time Consumption 94.38795161247253\n"
          ]
        }
      ],
      "source": [
        "lr = 1e-4\n",
        "optimizer = optim.Adam(model_mixer.parameters(), lr=lr)\n",
        "lowest_loss = 1e6\n",
        "count = 0\n",
        "time_start = time.time()\n",
        "for t in range( 25 ):\n",
        "    print(f\"-------------Epoch {t+1}-------------\")\n",
        "    print(\"current learning rate\", lr)\n",
        "    #optimizer = optim.SGD(model.parameters(), lr=lr_list[t], momentum=0.9)\n",
        "    train_loop(train_loader, model_mixer, criterion, optimizer)\n",
        "    test_loss = test_loop_Entropy(test_loader, model_mixer, criterion)\n",
        "    if int(test_loss * 10000) < lowest_loss:\n",
        "        lowest_loss = int(test_loss * 10000)\n",
        "        count = 0\n",
        "    else:\n",
        "        count += 1\n",
        "    if count == 3:\n",
        "        lr /= 10\n",
        "        optimizer = optim.Adam(model_mixer.parameters(), lr=lr)\n",
        "        count = 0\n",
        "print(\"Done!\")\n",
        "time_end = time.time()\n",
        "print(\"Time Consumption\",time_end-time_start)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"./gdrive/MyDrive/DeepLense_model/DeepLense_MLP_mixer_weights.pth\"\n",
        "torch.save(model_mixer.state_dict(), path)"
      ],
      "metadata": {
        "id": "VtE8txBGBraj"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA76Uf1UwDNb"
      },
      "source": [
        "### ROC-AUC (MLP-mixer, post-norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cc131ef-75dc-4772-9df6-d4a9e491d16d",
        "id": "jGjnwWw9wDNl"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process: 0.0\\%\n"
          ]
        }
      ],
      "source": [
        "total_prob_mixer, total_label_mixer = test_loop_prob(test_loader, model_mixer, class_i=0)\n",
        "fpr, tpr, thresholds = metrics.roc_curve(total_label_mixer, total_prob_mixer)\n",
        "mixer_AUC_1 = metrics.roc_auc_score(total_label_mixer, total_prob_mixer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc06e36d-e031-4be7-cfd3-1d9bb003e5e4",
        "id": "O8CnQicIwDNl"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ROC-AUC score of MLP-mixer: 0.972552\n"
          ]
        }
      ],
      "source": [
        "print(\"The ROC-AUC score of MLP-mixer:\", mixer_AUC_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "e624d915-9bf6-420e-e786-2a484b1e33f1",
        "id": "d7o8aDUewDNl"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(fpr, tpr, \"--\", color=\"magenta\")\n",
        "#plt.plot([0,1],[0,1],\"--\")\n",
        "plt.grid()\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC curve for MLP-mixer\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}